[["index.html", "GOVT 8002 Shared Working Book Benjamin Reese 1 Introduction 1.1 Purpose of This Site 1.2 Loading Data", " GOVT 8002 Shared Working Book Benjamin Reese Zoom Link for Office Hours | bfr11@georgetown.edu 1 Introduction 1.1 Purpose of This Site The goal of this site is to have a place where you can look for examples of code. Think of this site as a log of all of the topics we cover in class and in lab sessions. I will make updates throughout the semester as we cover more advanced material. I hope this proves to be a helpful and beneficial resource and offer an easier to access format than the Google Doc. Let me know if any of this code doesn’t run correctly or if you have any questions or issues! 1.2 Loading Data The easiest way to load data into R and ensure you have the correct file path is to create a folder on your computer for each assignment and place the datasets directly into that folder. Create a folder on your computer for each new analysis Download your Data and move the file to your newly created folder Then open RStudio Click the project button in the top right corner Click new project Click existing directory Click browse and find the folder that you created Click create project Once your new project opens, click the blank page with a green plus sign icon in the top left corner under the file option Click R script to open a new script You should also be able to see your data file in the bottom right window of RStudio, click the file and follow the options depending on the file type Once your data is imported into R, the code that R automatically ran will be in the console window on the bottom left, copy and paste it to your fresh R script For example, in Lab 1, my code looked like: read_excel(\"Data/USstates.xlsx\") Run this copy and pasted line of code whenever you open the R Project and you will never have to worry about complicated file pathing commands I recommend using the assignment operator &lt;- to give your dataset a short and simple name like df, dta, or, if you are working with multiple datasets, name each something short and descriptive "],["lab-i-tidyverse-ols-review.html", "2 9/23 | Lab I: Tidyverse &amp; OLS Review 2.1 Join the data sets. 2.2 Create and add the following variables to your dataframe density, deaths per capita, cases per capita and vaccinated percent. 2.3 Estimate three regression models with deaths per capita on your selected day as the dependent variable. 2.4 Assess specific vaccines 2.5 Optional Lab 1b", " 2 9/23 | Lab I: Tidyverse &amp; OLS Review # Preparation library(tidyverse) library(readr) library(readxl) library(stargazer) library(car) 2.1 Join the data sets. Join the cases and vaccination data by date and state. Case data: United_States_COVID-19_Cases_and_Deaths_by_State_over_Time.csv Vaccination data: COVID-19_Vaccinations_in_the_United_States_Jurisdiction.csv Other state variables: USstates.xlsx Add the USstates.xlsx data and limit your dataframe to the states listed in USstates.xlsx. How do you know if your merge was successful? ## Loading Case Data case &lt;- read_csv(&quot;Data/United_States_COVID-19_Cases_and_Deaths_by_State_over_Time.csv&quot;) ## Loading Vax Data vax &lt;- read_csv(&quot;Data/COVID-19_Vaccinations_in_the_United_States_Jurisdiction.csv&quot;) ## Joining Case &amp; Vax case_vax &lt;- case %&gt;% left_join(vax, by = c(&quot;submission_date&quot; = &quot;Date&quot;, &quot;state&quot; = &quot;Location&quot;)) ## Adding in state variables states &lt;- read_excel(&quot;Data/USstates.xlsx&quot;) ## Joining all datasets to create df df &lt;- case_vax %&gt;% filter(state %in% states$stateAbbr) %&gt;% left_join(states, by = c(&quot;state&quot; = &quot;stateAbbr&quot;)) %&gt;% janitor::clean_names() ## making variable names lowercase with underscores ## Checking for only states count(tibble(unique(df$state))) ## unique.df.state. freq ## 1 AK 1 ## 2 AL 1 ## 3 AR 1 ## 4 AZ 1 ## 5 CA 1 ## 6 CO 1 ## 7 CT 1 ## 8 DC 1 ## 9 DE 1 ## 10 FL 1 ## 11 GA 1 ## 12 HI 1 ## 13 IA 1 ## 14 ID 1 ## 15 IL 1 ## 16 IN 1 ## 17 KS 1 ## 18 KY 1 ## 19 LA 1 ## 20 MA 1 ## 21 MD 1 ## 22 ME 1 ## 23 MI 1 ## 24 MN 1 ## 25 MO 1 ## 26 MS 1 ## 27 MT 1 ## 28 NC 1 ## 29 ND 1 ## 30 NE 1 ## 31 NH 1 ## 32 NJ 1 ## 33 NM 1 ## 34 NV 1 ## 35 NY 1 ## 36 OH 1 ## 37 OK 1 ## 38 OR 1 ## 39 PA 1 ## 40 RI 1 ## 41 SC 1 ## 42 SD 1 ## 43 TN 1 ## 44 TX 1 ## 45 UT 1 ## 46 VA 1 ## 47 VT 1 ## 48 WA 1 ## 49 WI 1 ## 50 WV 1 ## 51 WY 1 ## View(df) To use janitor::clean_names() above, be sure to install the janitor package with install.packages(\"janitor\") 2.2 Create and add the following variables to your dataframe density, deaths per capita, cases per capita and vaccinated percent. For a specific day, show the top five states ranked by deaths per capita and calculate the average vaccinated per capita and the mean, minimum and maximum deaths per capita. ## Creating Variables df &lt;- df %&gt;% mutate(&quot;deaths_pc&quot; = 100000*new_death/pop2019, &quot;cases_pc&quot; = 100000*new_case/pop2019, &quot;vaxed_pct&quot; = series_complete_12plus_pop_pct/100, &quot;density&quot; = pop2019/(1000000*sq_miles)) ## Filtering to specific day df_day &lt;- df %&gt;% filter(submission_date == &quot;10/1/2021&quot;) ## Top five states by deaths per capita df_day %&gt;% arrange(desc(deaths_pc)) %&gt;% dplyr::select(submission_date, state, deaths_pc, cases_pc, vaxed_pct, density) %&gt;% slice(1:5) ## # A tibble: 5 × 6 ## submission_date state deaths_pc cases_pc vaxed_pct density ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 10/1/2021 WV 2.90 91.8 0.465 0.0000746 ## 2 10/1/2021 OH 2.81 53.3 0.588 0.000286 ## 3 10/1/2021 GA 1.49 31.5 0.537 0.000185 ## 4 10/1/2021 ID 1.34 99.5 0 0.0000216 ## 5 10/1/2021 ND 1.31 93.3 0.529 0.0000110 ## Mean vaxed per capita and mean, min, max of deaths per capita df_day %&gt;% summarise(mean_vaxed = mean(vaxed_pct), mean_deaths = mean(deaths_pc), min_deaths = min(deaths_pc), max_deaths = max(deaths_pc)) ## mean_vaxed mean_deaths min_deaths max_deaths ## 1 0.6248235 0.5513933 0 2.901548 2.3 Estimate three regression models with deaths per capita on your selected day as the dependent variable. Your first model will have only Trump 2020 percent as an independent variable. Your second model will add vaccinated percent as an independent variable. Your third model will add density. Before you estimate the models, write down your expectations about what will happen in these models. ## Trump percent model ols.1 &lt;- lm(deaths_pc ~ trump2020_percent, data = df_day) summary(ols.1) ## ## Call: ## lm(formula = deaths_pc ~ trump2020_percent, data = df_day) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.88445 -0.31999 -0.09294 0.17142 2.18890 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.2412 0.3422 -0.705 0.4843 ## trump2020_percent 1.6103 0.6759 2.382 0.0211 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.573 on 49 degrees of freedom ## Multiple R-squared: 0.1038, Adjusted R-squared: 0.08552 ## F-statistic: 5.676 on 1 and 49 DF, p-value: 0.02112 ## Trump and Vax model ols.2 &lt;- lm(deaths_pc ~ trump2020_percent + vaxed_pct, data = df_day) summary(ols.2) ## ## Call: ## lm(formula = deaths_pc ~ trump2020_percent + vaxed_pct, data = df_day) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.8757 -0.2596 -0.1054 0.1314 2.1706 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.5915 0.8539 1.864 0.0685 . ## trump2020_percent 0.3252 0.8514 0.382 0.7041 ## vaxed_pct -1.9208 0.8264 -2.324 0.0244 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5489 on 48 degrees of freedom ## Multiple R-squared: 0.1945, Adjusted R-squared: 0.1609 ## F-statistic: 5.795 on 2 and 48 DF, p-value: 0.005569 ## Trump, Vax, and Density Model ols.3 &lt;- lm(deaths_pc ~ trump2020_percent + vaxed_pct + density, data = df_day) summary(ols.3) ## ## Call: ## lm(formula = deaths_pc ~ trump2020_percent + vaxed_pct + density, ## data = df_day) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.8859 -0.2580 -0.1098 0.1301 2.1695 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.5157 1.0494 1.444 0.1553 ## trump2020_percent 0.4184 1.1311 0.370 0.7131 ## vaxed_pct -1.8785 0.8991 -2.089 0.0421 * ## density 8.2056 64.6874 0.127 0.8996 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5546 on 47 degrees of freedom ## Multiple R-squared: 0.1948, Adjusted R-squared: 0.1434 ## F-statistic: 3.789 on 3 and 47 DF, p-value: 0.01628 The results of ols.1, using October 1st, 2021 as the day, indicate that area with higher support for Donald Trump have more deaths per capita, specifically, a one percentage increase in Trump’s 2020 vote share, on average, increases deaths per capita by 1.6. This is a classic case of omitted variable bias, however. Once we control for the percent vaxed in an area, the trump2020_percent loses statistical significance. We can see in the second model that the coefficient on the Trump vote share variable greatly decreases and is no longer statistically significant at conventional thresholds. vaxed_pct is statistically significant at the \\(p&lt;.05\\) level and indicates that a one percent increase in an areas vaccination rate corresponds, on average, with a 1.9 decrease in deaths per capita, holding trump2020_percent constant. This result stands even when controlling for an areas density as shown in model 3. 2.4 Assess specific vaccines Create and add vaccinated percent by state for each of the Pfizer, Modern and Janssen (which is Johnson and Johnson) vaccines. Use pop2019 for population Use Series_Complete_Moderna_18Plus, Series_Complete_Janssen_18Plus and Series_Complete_Pfizer_18Plus for the vaccination totals. Estimate a model in which deaths per capita is a function of all three vaccination rates. Explain what the results mean, especially in light of the results above for overall vaccination results. Explain how one would compare the efficacy of the individual vaccines (e.g., whether the Moderna vaccine works better than the Johnson and Johnson vaccine). Many answers would work here. ## Creating vaxed percent variables for different vaccines df &lt;- df %&gt;% mutate(vaxed_pct_moderna = series_complete_moderna_18plus/(100*pop2019), vaxed_pct_jans = series_complete_janssen_18plus/(100*pop2019), vaxed_pct_pfizer = series_complete_pfizer_18plus/(100*pop2019)) ## Filtering to day df_day = df %&gt;% filter(submission_date == &quot;10/1/2021&quot;) ## Running Model ols.4 &lt;- lm(deaths_pc ~ vaxed_pct_jans + vaxed_pct_moderna + vaxed_pct_pfizer, data = df_day) summary(ols.4) ## ## Call: ## lm(formula = deaths_pc ~ vaxed_pct_jans + vaxed_pct_moderna + ## vaxed_pct_pfizer, data = df_day) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.93880 -0.29403 -0.05261 0.15284 2.13378 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.373 0.679 3.496 0.00104 ** ## vaxed_pct_jans -570.304 1174.009 -0.486 0.62938 ## vaxed_pct_moderna -359.090 586.494 -0.612 0.54331 ## vaxed_pct_pfizer -321.407 333.845 -0.963 0.34060 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5544 on 47 degrees of freedom ## Multiple R-squared: 0.1954, Adjusted R-squared: 0.1441 ## F-statistic: 3.806 on 3 and 47 DF, p-value: 0.01599 Lab I OLS Results Model 1 Model 2 Model 3 Model 4 (1) (2) (3) (4) trump2020_percent 1.610 0.325 0.418 (0.676) (0.851) (1.131) t = 2.382 t = 0.382 t = 0.370 vaxed_pct -1.921 -1.879 (0.826) (0.899) t = -2.324 t = -2.089 density 8.206 (64.687) t = 0.127 vaxed_pct_jans -570.304 (1,174.009) t = -0.486 vaxed_pct_moderna -359.090 (586.494) t = -0.612 vaxed_pct_pfizer -321.407 (333.845) t = -0.963 Constant -0.241 1.591 1.516 2.374 (0.342) (0.854) (1.049) (0.679) t = -0.705 t = 1.864 t = 1.444 t = 3.496 Observations 51 51 51 51 R2 0.104 0.194 0.195 0.195 Residual Std. Error 0.573 (df = 49) 0.549 (df = 48) 0.555 (df = 47) 0.554 (df = 47) F Statistic 5.676** (df = 1; 49) 5.795*** (df = 2; 48) 3.789** (df = 3; 47) 3.806** (df = 3; 47) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 The results are no longer statistically significance due to multicollinearity. There is a high degree of correlation between the rates each vaccine were administered which is contributing to the large standard errors. The p-value of the \\(F\\)-statistic tells us the probability that we would see a result this extreme given all of the coefficients on the vaccines are actually zero, reinforcing our findings above. This is not necessary for this lab, but to assess the efficacy of one vaccine over another, we could conduct an \\(F\\) test. Remember, the formula for F-tests is: \\[F_{q, N-k} = \\frac{(R^2_{Unrestricted}-R^2_{Restricted})\\setminus q}{(1-R^2_{Unrestricted}) \\setminus (N-k)}\\] As an example, I will evaluate if \\(\\beta_1 = \\beta_2\\) in the following model: \\(deaths_pc_{it} = \\beta_0 + \\beta_1jans_{it} + \\beta_2moderna_{it} + \\beta_3pfizer_{it} + \\epsilon_i\\) which is ols.4 estimated above that examines the relationship between the rate of vaccination for each vaccine and deaths per capita in state \\(i\\) and day \\(t\\). We can do this manually, but library(car) makes it easy. See the code below: ## F Test linearHypothesis(ols.4, c(&quot;vaxed_pct_jans = vaxed_pct_moderna&quot;)) ## Linear hypothesis test ## ## Hypothesis: ## vaxed_pct_jans - vaxed_pct_moderna = 0 ## ## Model 1: restricted model ## Model 2: deaths_pc ~ vaxed_pct_jans + vaxed_pct_moderna + vaxed_pct_pfizer ## ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 48 14.451 ## 2 47 14.445 1 0.0056829 0.0185 0.8924 We can see that the \\(p\\)-value is not statistically significant, meaning that we cannot reject the null hypothesis that \\(\\beta_1\\) is statistically significantly different than \\(\\beta_2\\). 2.5 Optional Lab 1b This lab is optional and just for some additional practice. 2.5.1 Load the World Values Survey data from “Data/WV7_small.csv” and the CountryCode data from “Data/Country codes for WVS wave 7.csv”. The codebook for the World Values Survey data is available at http://www.worldvaluessurvey.org/WVSDocumentationWV7.jsp The variables in WV_small were created as follows: Satisfaction with your life from 1 (completely dissatisfied) to 10 (completely satisfied) (V23) Income: a country-specific scale ranging from 1 (lowest income category) to 10 (highest income category) (V239 in the dataset) Education: a scale ranging from 1 (no formal education) to 9 (a degree from a university) (V248 in the dataset) Country: based on V2 in the dataset. See “Country codes for WVS wave 6.csv” Conservatism: self-identified political conservatism from 1 (most liberal) to 10 (most conservative) (V95 in the data set) Male: a dummy variable indicating male (V240) Religious: Indicating how often the individual attends religious services ranging from 1 (almost never) to 7 (more than once a week) (this is a re-coding of V145 that had the polarity reversed, but is harder to interpret) Marital: Marital status 1 = Married 2 = Living together as married 3 = Divorced 4 = Separated 5 = Widowed 6 = Single Survey year: year survey taken Birth year: respondent’s year of birth ## World Values Dataset wv &lt;- read_csv(&quot;Data/WV7_small.csv&quot;) ## Country Codes cc &lt;- read_csv(&quot;Data/Country codes for WVS wave 7.csv&quot;) 2.5.2 Join the country name from the CountryCode data to the World Values data. The country code is V2 in CountryCode and B_Country in the World Values data. Create two data objects of the countries in and not in the World Values data. Display the first 10 countries for each list. ## Left Join wv &lt;- wv %&gt;% left_join(cc, by = c(&quot;B_COUNTRY&quot; = &quot;V2&quot;)) ## Showing countries in countries_in_wv &lt;- tibble(unique(wv$country)) countries_in_wv[1:10,] ## # A tibble: 10 × 1 ## `unique(wv$country)` ## &lt;chr&gt; ## 1 Ethiopia ## 2 Zimbabwe ## 3 Tajikistan ## 4 Pakistan ## 5 Bangladesh ## 6 Nigeria ## 7 Myanmar ## 8 Kyrgyzstan ## 9 Nicaragua ## 10 Viet Nam ## Anti Join countries_out &lt;- cc %&gt;% anti_join(wv, by = c(&quot;V2&quot; = &quot;B_COUNTRY&quot;)) %&gt;% filter(V2 &gt; 0) ## Countries not in wv countries_out %&gt;% slice(1:10) ## # A tibble: 10 × 2 ## V2 country ## &lt;dbl&gt; &lt;chr&gt; ## 1 8 Albania ## 2 12 Algeria ## 3 16 American Samoa ## 4 24 Angola ## 5 28 Antigua ## 6 31 Azerbaijan ## 7 40 Austria ## 8 48 Bahrain ## 9 51 Armenia ## 10 52 Barbados 2.5.3 Using pipes, calculate the percent immigrant in every country. Show the highest 10 and lowest 10 countries. (Be sure to think through what the variable in the data set means.) ## Recoding Immigrant Variable wv$Immigrant &lt;- wv$Immigrant-1 ## 10 Highest wv %&gt;% group_by(country) %&gt;% dplyr::summarize(pct_imm = mean(Immigrant, na.rm=T), n=n())%&gt;% arrange(desc(pct_imm)) %&gt;% slice(1:10) ## # A tibble: 10 × 3 ## country pct_imm n ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Andorra 0.723 1004 ## 2 Macao 0.417 1023 ## 3 Hong Kong 0.261 2075 ## 4 New Zealand 0.227 1057 ## 5 China 0.221 3036 ## 6 Cyprus (G) 0.22 1000 ## 7 Germany 0.139 1528 ## 8 Jordan 0.138 1203 ## 9 Greece 0.106 1200 ## 10 United States 0.106 2596 ## 10 Lowest ## Percent immigrant in each country wv %&gt;% group_by(country) %&gt;% dplyr::summarize(pct_imm = mean(Immigrant, na.rm=T), n=n())%&gt;% arrange(pct_imm) %&gt;% slice(1:10) ## # A tibble: 10 × 3 ## country pct_imm n ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Bangladesh 0 1200 ## 2 Iraq 0 1200 ## 3 Mexico 0 1739 ## 4 Myanmar 0 1200 ## 5 Thailand 0 1500 ## 6 Viet Nam 0 1200 ## 7 Egypt 0.000833 1200 ## 8 Indonesia 0.000938 3200 ## 9 Romania 0.00159 1257 ## 10 Ethiopia 0.00163 1230 2.5.4 We’re now going to give some examples of the kind of data we can calculate. Think about how you would do this in Excel (but don’t actually do it!) and then calculate in R. Calculate the percent of immigrants who are men, by country and show the highest and lowest 10 countries. ## 10 Highest wv %&gt;% filter(Male==1) %&gt;% group_by(country) %&gt;% dplyr::summarize(pct_imm = mean(Immigrant, na.rm=T), n=n()) %&gt;% arrange(desc(pct_imm)) %&gt;% slice(1:10) ## # A tibble: 10 × 3 ## country pct_imm n ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Andorra 0.784 495 ## 2 Macao 0.443 572 ## 3 Hong Kong 0.257 1123 ## 4 China 0.223 1667 ## 5 New Zealand 0.220 594 ## 6 Cyprus (G) 0.191 518 ## 7 Jordan 0.154 596 ## 8 Germany 0.145 785 ## 9 Greece 0.111 637 ## 10 United States 0.103 1206 ## 10 Lowest wv %&gt;% filter(Male==1) %&gt;% group_by(country) %&gt;% dplyr::summarize(pct_imm = mean(Immigrant, na.rm=T), n=n()) %&gt;% arrange(pct_imm) %&gt;% slice(1:10) ## # A tibble: 10 × 3 ## country pct_imm n ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Bangladesh 0 608 ## 2 Egypt 0 579 ## 3 Iraq 0 592 ## 4 Mexico 0 866 ## 5 Myanmar 0 599 ## 6 Nigeria 0 604 ## 7 Thailand 0 793 ## 8 Viet Nam 0 655 ## 9 Indonesia 0.00114 1754 ## 10 Romania 0.00132 760 2.5.5 Group by marital status and country and show the percent of people in each category who are men. Show the results for a country of your choice and briefly discuss what it means when the percentages in each category are not roughly 50 percent. Recall that the coding for the Marital variable is 1 = Married 2 = Living together as married 3 = Divorced 4 = Separated 5 = Widowed 6 = Single. wv %&gt;% group_by(country, Marital) %&gt;% dplyr::summarize(male_pct = mean(Male, na.rm=T), n=n()) %&gt;% filter(country == &quot;United States&quot;) ## # A tibble: 7 × 4 ## # Groups: country [1] ## country Marital male_pct n ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 United States 1 0.431 1296 ## 2 United States 2 0.548 197 ## 3 United States 3 0.493 278 ## 4 United States 4 0.5 54 ## 5 United States 5 0.560 84 ## 6 United States 6 0.477 677 ## 7 United States NA 0.6 10 2.5.6 Come up with your own alternatives: think about some possible subset of people and some information we have about them and see if you can write code to capture that information. wv %&gt;% group_by(country, Income) %&gt;% dplyr::summarize(imm_pct = mean(Immigrant, na.rm=T), n=n()) %&gt;% filter(country == &quot;United States&quot;) ## # A tibble: 11 × 4 ## # Groups: country [1] ## country Income imm_pct n ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 United States 1 0.106 123 ## 2 United States 2 0.072 128 ## 3 United States 3 0.0871 268 ## 4 United States 4 0.0867 373 ## 5 United States 5 0.110 599 ## 6 United States 6 0.111 468 ## 7 United States 7 0.110 370 ## 8 United States 8 0.142 151 ## 9 United States 9 0.111 36 ## 10 United States 10 0.227 22 ## 11 United States NA 0.136 58 "],["lab-ii-dummy-variables-career-data.html", "3 9/30 | Lab II: Dummy Variables &amp; Career Data 3.1 Use OLS to estimate the difference in happiness (the “happy” variable) for married versus unmarried people. Then use the t test function to assess the difference in happiness by gender (see the Computing Corner in Chapter 6 of the book). Discuss similarities and differences in OLS model and t test function results 3.2 Use OLS with robust standard errors to estimate the difference in happiness for married and unmarried people. Then use the t test function with unequal variances to assess the difference in happiness by marital status. Discuss similarities and differences in OLS model and t test function results. 3.3 Create an interaction between married and age. Estimate a model that explains happiness as a function of age and marital status, allowing for the age effect to differ according to marital status. (For simplicity, we use only married and unmarried for marital status.) What is the effect of age for unmarried people? For married people? 3.4 Estimate separate models explaining happiness in terms of age for married and unmarried people. Comment on similarities and differences compared to results immediately above for (i) the estimated effect of age and (ii) the intercept. 3.5 Marianne Bertrand wrote an article called ``Work on Women’s Work is Never Done: Career, Family, and the Well-Being of College-Educated Women’’ published in the American Economic Review: Papers &amp; Proceedings 2013, 103(3): 244–250. She analyzed the effect of careers and family on college-educated women. She defines a career variable that is 1 if someone’s earnings are above the twenty-fifth percentile in the relevant year and age group. Estimate a model in which happiness is a function of career, being married and an interaction of career and married (“careermarried”). To match Betrand’s analysis, limit the data to only to college educated (dta$educat==4) women (dta$sex==2). Interpret the estimated average happiness for the four types of women implied by this analysis. 3.6 The GSS provides a race variable (dta$race). The variable equals 1 for white respondents, 2 for black respondents and 3 for everyone else. Add race to the above model and interpret the coefficients related to race. Estimate another model with a different reference category and explain coefficients across the two models. 3.7 Estimate a model similar to the above, but for a different group of your choice (e.g., limit by race or gender or education) and feel free to include different covariates.", " 3 9/30 | Lab II: Dummy Variables &amp; Career Data ## Packages library(knitr) library(car) library(AER) library(stargazer) ## Data load(&quot;~/GOVT702/Data/Ch6_Lab_CareerHappiness.RData&quot;) We will use General Social Survey data for this lab. The key variables we will use are happy: 3 very happy, 2 pretty happy, 1 not too happy married: 1 for currently married (based on dta$marital, which is 1 for married, 2 for widowed, 3 for divorced, 4 for separated, 5 for never married) sex: 1 for men, 2 for women edcat: 1 for less than high school, 2 for high school, 3 for some college and 4 for college graduate race: 1 for white, 2 for black, 3 for other races (self-identified, first race mentioned) 3.1 Use OLS to estimate the difference in happiness (the “happy” variable) for married versus unmarried people. Then use the t test function to assess the difference in happiness by gender (see the Computing Corner in Chapter 6 of the book). Discuss similarities and differences in OLS model and t test function results ## Difference in Happiness with OLS model_1 &lt;- lm(happy~married, data=dta) summary(model_1) ## ## Call: ## lm(formula = happy ~ married, data = dta) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.3227 -0.3227 -0.0249 0.6773 0.9751 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.024903 0.006033 335.64 &lt;2e-16 *** ## married 0.297810 0.007621 39.08 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5984 on 26346 degrees of freedom ## (2442 observations deleted due to missingness) ## Multiple R-squared: 0.05478, Adjusted R-squared: 0.05475 ## F-statistic: 1527 on 1 and 26346 DF, p-value: &lt; 2.2e-16 confint(model_1) ## 2.5 % 97.5 % ## (Intercept) 2.013079 2.0367282 ## married 0.282872 0.3127481 ## With t.test() t.test(dta$happy[dta$married==1], dta$happy[dta$married==0]) ## ## Welch Two Sample t-test ## ## data: dta$happy[dta$married == 1] and dta$happy[dta$married == 0] ## t = 38.977, df = 20513, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.2828336 0.3127865 ## sample estimates: ## mean of x mean of y ## 2.322714 2.024903 3.2 Use OLS with robust standard errors to estimate the difference in happiness for married and unmarried people. Then use the t test function with unequal variances to assess the difference in happiness by marital status. Discuss similarities and differences in OLS model and t test function results. ## model_1 with Robust Standard Errors coeftest(model_1, vcov. = vcovHC(model_1, type = &quot;HC1&quot;)) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.0249034 0.0060709 333.541 &lt; 2.2e-16 *** ## married 0.2978101 0.0076407 38.977 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # t-test, without assuming equal variance t.test(dta$happy[dta$married==1], dta$happy[dta$married==0], var.equal = FALSE) ## ## Welch Two Sample t-test ## ## data: dta$happy[dta$married == 1] and dta$happy[dta$married == 0] ## t = 38.977, df = 20513, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.2828336 0.3127865 ## sample estimates: ## mean of x mean of y ## 2.322714 2.024903 3.3 Create an interaction between married and age. Estimate a model that explains happiness as a function of age and marital status, allowing for the age effect to differ according to marital status. (For simplicity, we use only married and unmarried for marital status.) What is the effect of age for unmarried people? For married people? ## Interaction Term dum_interact &lt;- dta$married*dta$age ## Model 2 model_2 &lt;- lm(happy~age + married + dum_interact, data=dta) summary(model_2) ## ## Call: ## lm(formula = happy ~ age + married + dum_interact, data = dta) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.32925 -0.32408 -0.04825 0.67451 1.02224 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.1300310 0.0270265 78.813 &lt; 2e-16 *** ## age -0.0028199 0.0007067 -3.990 6.61e-05 *** ## married 0.2109524 0.0351041 6.009 1.89e-09 *** ## dum_interact 0.0023504 0.0009036 2.601 0.0093 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5982 on 26344 degrees of freedom ## (2442 observations deleted due to missingness) ## Multiple R-squared: 0.05538, Adjusted R-squared: 0.05527 ## F-statistic: 514.8 on 3 and 26344 DF, p-value: &lt; 2.2e-16 ## or model_2b &lt;- lm(happy~married*age, data=dta) summary(model_2b) ## ## Call: ## lm(formula = happy ~ married * age, data = dta) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.32925 -0.32408 -0.04825 0.67451 1.02224 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.1300310 0.0270265 78.813 &lt; 2e-16 *** ## married 0.2109524 0.0351041 6.009 1.89e-09 *** ## age -0.0028199 0.0007067 -3.990 6.61e-05 *** ## married:age 0.0023504 0.0009036 2.601 0.0093 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5982 on 26344 degrees of freedom ## (2442 observations deleted due to missingness) ## Multiple R-squared: 0.05538, Adjusted R-squared: 0.05527 ## F-statistic: 514.8 on 3 and 26344 DF, p-value: &lt; 2.2e-16 The effect of age is \\(-0.00281\\) for unmarried people and \\(-0.00281 + 0.002350 = -0.0004699\\) for married people. The intercept is \\(2.13\\) for unmarried people and \\(2.13 + 0.2109 = 2.3409\\) for married people. 3.4 Estimate separate models explaining happiness in terms of age for married and unmarried people. Comment on similarities and differences compared to results immediately above for (i) the estimated effect of age and (ii) the intercept. ## Model 3a model_3a &lt;- lm(happy~age, data=subset(dta, married==1)) summary(model_3a) ## ## Call: ## lm(formula = happy ~ age, data = subset(dta, married == 1)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.3293 -0.3250 -0.3189 0.6754 0.6844 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.3409834 0.0223240 104.864 &lt;2e-16 *** ## age -0.0004695 0.0005612 -0.837 0.403 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5961 on 16508 degrees of freedom ## (1385 observations deleted due to missingness) ## Multiple R-squared: 4.24e-05, Adjusted R-squared: -1.817e-05 ## F-statistic: 0.7 on 1 and 16508 DF, p-value: 0.4028 ## and model_3b &lt;- lm(happy~age, data=subset(dta, married==0)) summary(model_3b) ## ## Call: ## lm(formula = happy ~ age, data = subset(dta, married == 0)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.05953 -0.05389 -0.02570 0.01096 1.02224 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.1300310 0.0271844 78.355 &lt; 2e-16 *** ## age -0.0028199 0.0007108 -3.967 7.32e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6017 on 9836 degrees of freedom ## (1053 observations deleted due to missingness) ## Multiple R-squared: 0.001598, Adjusted R-squared: 0.001496 ## F-statistic: 15.74 on 1 and 9836 DF, p-value: 7.322e-05 The results here are the same as above: for unmarried people, the effect of age is \\(-0.00281\\) and the intercept is \\(2.130\\). The effect of age for married people also the same: \\(-0.000469\\), with an intercept of \\(2.3409\\). 3.5 Marianne Bertrand wrote an article called ``Work on Women’s Work is Never Done: Career, Family, and the Well-Being of College-Educated Women’’ published in the American Economic Review: Papers &amp; Proceedings 2013, 103(3): 244–250. She analyzed the effect of careers and family on college-educated women. She defines a career variable that is 1 if someone’s earnings are above the twenty-fifth percentile in the relevant year and age group. Estimate a model in which happiness is a function of career, being married and an interaction of career and married (“careermarried”). To match Betrand’s analysis, limit the data to only to college educated (dta$educat==4) women (dta$sex==2). Interpret the estimated average happiness for the four types of women implied by this analysis. model_4 &lt;- lm(happy ~ career*married, data=dta[dta$sex==2 &amp; dta$educat==4,]) summary(model_4) ## ## Call: ## lm(formula = happy ~ career * married, data = dta[dta$sex == ## 2 &amp; dta$educat == 4, ]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.4215 -0.4214 -0.1247 0.5786 0.8753 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.12470 0.01997 106.406 &lt; 2e-16 *** ## career 0.09204 0.02985 3.084 0.00206 ** ## married 0.29676 0.02503 11.855 &lt; 2e-16 *** ## career:married -0.09923 0.04023 -2.466 0.01369 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5739 on 3595 degrees of freedom ## (400 observations deleted due to missingness) ## Multiple R-squared: 0.04765, Adjusted R-squared: 0.04685 ## F-statistic: 59.96 on 3 and 3595 DF, p-value: &lt; 2.2e-16 The four groups are unmarried women without careers, married women with careers, married women without careers, and unmarried women without careers. College-educated women that have no careers and are not married have an average happiness of \\(2.12\\). College-educated women with careers but not married have an average happiness of \\(2.12 + 0.09 = 2.21\\). College-educated women with no career but are married have average happiness of \\(2.12 + 0.29 = 2.41\\). College-educated women that are both married and have careers have average happiness of \\(2.12 + 0.092 + 0.29 -0.099 = 2.4\\). 3.6 The GSS provides a race variable (dta$race). The variable equals 1 for white respondents, 2 for black respondents and 3 for everyone else. Add race to the above model and interpret the coefficients related to race. Estimate another model with a different reference category and explain coefficients across the two models. ## First Race Model with White as Reference model_5 &lt;- lm(happy ~ career*married + factor(race), data=dta[dta$sex==2 &amp; dta$educat==4,]) summary(model_5) ## ## Call: ## lm(formula = happy ~ career * married + factor(race), data = dta[dta$sex == ## 2 &amp; dta$educat == 4, ]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.4359 -0.4329 -0.1575 0.5641 1.0085 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.15746 0.02077 103.867 &lt; 2e-16 *** ## career 0.08830 0.02974 2.969 0.00301 ** ## married 0.27847 0.02518 11.060 &lt; 2e-16 *** ## factor(race)2 -0.16593 0.03267 -5.079 3.98e-07 *** ## factor(race)3 -0.10051 0.04036 -2.491 0.01280 * ## career:married -0.09132 0.04010 -2.277 0.02283 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5717 on 3593 degrees of freedom ## (400 observations deleted due to missingness) ## Multiple R-squared: 0.05555, Adjusted R-squared: 0.05424 ## F-statistic: 42.27 on 5 and 3593 DF, p-value: &lt; 2.2e-16 ## Second Race Model with Other as the Reference Category model_5b &lt;- lm(happy ~ career*married + relevel(factor(race), ref = &quot;3&quot;), data=dta[dta$sex==2 &amp; dta$educat==4,]) summary(model_5b) ## ## Call: ## lm(formula = happy ~ career * married + relevel(factor(race), ## ref = &quot;3&quot;), data = dta[dta$sex == 2 &amp; dta$educat == 4, ]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.4359 -0.4329 -0.1575 0.5641 1.0085 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.05695 0.04261 48.273 &lt; 2e-16 *** ## career 0.08830 0.02974 2.969 0.00301 ** ## married 0.27847 0.02518 11.060 &lt; 2e-16 *** ## relevel(factor(race), ref = &quot;3&quot;)1 0.10051 0.04036 2.491 0.01280 * ## relevel(factor(race), ref = &quot;3&quot;)2 -0.06542 0.04972 -1.316 0.18835 ## career:married -0.09132 0.04010 -2.277 0.02283 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5717 on 3593 degrees of freedom ## (400 observations deleted due to missingness) ## Multiple R-squared: 0.05555, Adjusted R-squared: 0.05424 ## F-statistic: 42.27 on 5 and 3593 DF, p-value: &lt; 2.2e-16 Both results indicated white college-educated women are both \\(0.16\\) higher on the happiness variable than African-American college-educated women and \\(0.10\\) higher than other college-educated women of other races. Even though the coefficient on the other race variables change across specifications, our interpretation of the results does not change; the change is due solely to the change in reference category. 3.7 Estimate a model similar to the above, but for a different group of your choice (e.g., limit by race or gender or education) and feel free to include different covariates. No wrong answers here (mostly) This model explores the how commute times affect happiness. I specifically think that time commuting will - conditionally - lower a person’s happiness if it means they are away from their families. Because of this, I create an interaction term between commuting and marriage and estimate the effects. ## Commuting Model on Adults model_6 &lt;- lm(happy ~ commute*married, data=dta[dta$age&gt;18,]) summary(model_6) ## ## Call: ## lm(formula = happy ~ commute * married, data = dta[dta$age &gt; ## 18, ]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.30339 -0.29480 -0.05136 0.69947 0.98484 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.0513587 0.0553465 37.064 &lt; 2e-16 *** ## commute -0.0003732 0.0020973 -0.178 0.858816 ## married 0.2520331 0.0681019 3.701 0.000231 *** ## commute:married -0.0001996 0.0026100 -0.076 0.939047 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5647 on 726 degrees of freedom ## (28060 observations deleted due to missingness) ## Multiple R-squared: 0.04227, Adjusted R-squared: 0.03831 ## F-statistic: 10.68 on 3 and 726 DF, p-value: 7.076e-07 My theory is not supported by this model. I found no significant effect for those who commute longer and no significant conditional effect of happiness for those who commute long hours and are married. I did find support that marriage in general makes a person happier, though this should not be considered a causal effect. "],["lab-iii-panel-data.html", "4 2/6 | Lab III: Panel Data 4.1 Load the data from OxCGRT_latest.csv 4.2 Data Organization 4.3 Use the lag function in dplyr to create lagged variables for cases and deaths. Also create “difference” (e.g., dCases) that is the change in cases for each state by date. Check to make sure it worked. 4.4 Merge the above data frame to data in USstates.xlsx 4.5 Write in notation and estimate a pooled model of total cases as a function of state policy. Discuss possible bias. 4.6 Write in notation and estimate a one-way fixed effect model where the fixed effect is state. Are you still worried about bias? 4.7 Write in notation and estimate a two-way fixed effect model where the fixed effects are state and date. Does this model address the source of bias identified earlier? 4.8 Test a model with robust-clustered standard errors. Do the results change at all? 4.9 Load in the Texas School Board Data and take a look at the variables. The next few questions are from exercise 5 in Bailey (2021). The basic thoery of the paper is that teachers unions will have less influence on school board members when turnout is greater since a wider swath of voters (non-teachers) will be voting. You can read the abstract below. 4.10 Write in notation and estimate the pooled model of the effect of OnCycle on LnAvgSalary. 4.11 Write in notation and estimate a difference-in-difference model of the effect of OnCycle on LnAvgSalary. What is the key coefficient that we are interested in?", " 4 2/6 | Lab III: Panel Data # Load packages used in this session of R library(knitr) library(tidyverse) library(lubridate) library(fixest) library(stringr) library(readxl) In this lab we will estimate standard panel data models on covid policy and cases/deaths in U.S. states. This is not a full-fledged analysis, but rather an initial exploration of the data that illustrates how fixed effects models work. We will also implement a difference-in-difference model to measure the effect of changing the time of an election on teacher salaries. 4.1 Load the data from OxCGRT_latest.csv Oxford provides data on covid deaths/cases and policy variables by day by state. For more background, see this data archive or this story that uses the data. We will use a variable called GovernmentResponseIndex. For details, see this. (And feel free to experiment with the other measures.) 4.2 Data Organization Load the OxCGRT_latest.csv data Limit it to U.S. states and DC (CountryName == \"United States\" &amp; RegionName!=\"NA\" &amp; RegionCode!=\"US_VI\") Select the following variables: RegionName, RegionCode, Date, GovernmentResponseIndex_Average, ConfirmedCases and ConfirmedDeaths Add a variable to this data frame using the following code (this variable will help us when merging below) ## Loading Covid Data covid &lt;- read_csv(&quot;Data/OxCGRT_USA_latest.csv&quot;) ## Cleaning &amp; Wrangling Covid Data df &lt;- covid %&gt;% filter(CountryName == &quot;United States&quot; &amp; RegionName!=&quot;NA&quot; &amp; RegionCode!=&quot;US_VI&quot;) %&gt;% mutate(Date=ymd(Date), GovernmentResponseIndex = GovernmentResponseIndex_Average) %&gt;% dplyr::select(RegionName, RegionCode, Date, GovernmentResponseIndex, ConfirmedCases, ConfirmedDeaths) %&gt;% mutate(&quot;RegionCode&quot; = str_replace_all(string = RegionCode, pattern = &quot;US_&quot;, replacement = &quot;&quot; )) Show the first three variables of the first three lines of this data frame. head(df[1:3,1:3]) ## # A tibble: 3 × 3 ## RegionName RegionCode Date ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; ## 1 Alaska AK 2020-01-01 ## 2 Alaska AK 2020-01-02 ## 3 Alaska AK 2020-01-03 4.3 Use the lag function in dplyr to create lagged variables for cases and deaths. Also create “difference” (e.g., dCases) that is the change in cases for each state by date. Check to make sure it worked. ## Creating lagged and change in cases df2 &lt;- df %&gt;% group_by(RegionCode) %&gt;% mutate(laggedcase = dplyr::lag(ConfirmedCases, order_by = Date), laggeddeaths = dplyr::lag(ConfirmedDeaths, order_by = Date), dcases = (ConfirmedCases-laggedcase), ddeaths = (ConfirmedDeaths-laggeddeaths)) 4.4 Merge the above data frame to data in USstates.xlsx Merge by state abbreviation Create percapita measures of change in deaths and cases (e.g. “deathsPC” = 10000*dDeaths/pop2019). ## Reading in State Data states &lt;- read_excel(&quot;Data/USstates.xlsx&quot;) ## Joining and creating change in deaths and cases per capita df3 &lt;- df2 %&gt;% left_join(states, by = c(&quot;RegionCode&quot; = &quot;stateAbbr&quot;)) %&gt;% mutate(deaths_pc = 10000*ddeaths/pop2019, cases_pc = 10000*dcases/pop2019) 4.5 Write in notation and estimate a pooled model of total cases as a function of state policy. Discuss possible bias. The model is: \\[confirmed\\_cases_{it} = \\beta_0 + \\beta_1average\\_response\\_index_{it} + \\epsilon_{it}\\] ## Pooled Model pooled_covid_model &lt;- lm(ConfirmedCases~GovernmentResponseIndex, data = df3) summary(pooled_covid_model) ## ## Call: ## lm(formula = ConfirmedCases ~ GovernmentResponseIndex, data = df3) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1632719 -624780 -388606 191235 10944694 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1730790 20055 86.30 &lt;2e-16 *** ## GovernmentResponseIndex -18824 409 -46.03 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1369000 on 54823 degrees of freedom ## (1071 observations deleted due to missingness) ## Multiple R-squared: 0.0372, Adjusted R-squared: 0.03718 ## F-statistic: 2118 on 1 and 54823 DF, p-value: &lt; 2.2e-16 4.6 Write in notation and estimate a one-way fixed effect model where the fixed effect is state. Are you still worried about bias? The model is: \\[confirmed\\_cases_{it} = \\beta_0 + \\beta_1average\\_response\\_index_{it} + \\alpha_{i} + \\epsilon_{it}\\] ## One-Way Model one_way &lt;- feols(ConfirmedCases~GovernmentResponseIndex | RegionCode, data = df3, vcov = &quot;iid&quot;) ## NOTE: 1,071 observations removed because of NA values (LHS: 1,071, RHS: 1,071). summary(one_way) ## OLS estimation, Dep. Var.: ConfirmedCases ## Observations: 54,825 ## Fixed-effects: RegionCode: 51 ## Standard-errors: IID ## Estimate Std. Error t value Pr(&gt;|t|) ## GovernmentResponseIndex -22865.8 297.783 -76.7867 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## RMSE: 966,135.8 Adj. R2: 0.520077 ## Within R2: 0.097186 4.7 Write in notation and estimate a two-way fixed effect model where the fixed effects are state and date. Does this model address the source of bias identified earlier? \\[confirmed\\_cases_{it} = \\beta_0 + \\beta_1average\\_response\\_index_{it} + \\alpha_{i} + \\tau_{t} + \\epsilon_{it}\\] ## Two-way Model two_way &lt;- feols(ConfirmedCases~GovernmentResponseIndex | RegionCode + Date, data = df3, vcov = &quot;iid&quot;) ## NOTE: 1,071 observations removed because of NA values (LHS: 1,071, RHS: 1,071). summary(two_way) ## OLS estimation, Dep. Var.: ConfirmedCases ## Observations: 54,825 ## Fixed-effects: RegionCode: 51, Date: 1,075 ## Standard-errors: IID ## Estimate Std. Error t value Pr(&gt;|t|) ## GovernmentResponseIndex -4524.85 745.754 -6.06749 1.3079e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## RMSE: 760,870.0 Adj. R2: 0.696389 ## Within R2: 6.851e-4 4.8 Test a model with robust-clustered standard errors. Do the results change at all? ## Two-way FE with robust-clustered se two_way_robust &lt;- feols(ConfirmedCases~GovernmentResponseIndex | RegionCode + Date, data = df3, vcov = &quot;twoway&quot;) ## NOTE: 1,071 observations removed because of NA values (LHS: 1,071, RHS: 1,071). summary(two_way_robust) ## OLS estimation, Dep. Var.: ConfirmedCases ## Observations: 54,825 ## Fixed-effects: RegionCode: 51, Date: 1,075 ## Standard-errors: Clustered (RegionCode &amp; Date) ## Estimate Std. Error t value Pr(&gt;|t|) ## GovernmentResponseIndex -4524.85 13438.6 -0.336705 0.73775 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## RMSE: 760,870.0 Adj. R2: 0.696389 ## Within R2: 6.851e-4 Yes, the standard errors are much larger. 4.9 Load in the Texas School Board Data and take a look at the variables. The next few questions are from exercise 5 in Bailey (2021). The basic thoery of the paper is that teachers unions will have less influence on school board members when turnout is greater since a wider swath of voters (non-teachers) will be voting. You can read the abstract below. Paper Abstract: Many governments in the United States hold elections on days other than national Election Day. Recent studies have argued that the low voter turnout that accompanies such off-cycle elections could create an advantage for interest groups. However, the endogeneity of election timing makes it difficult to estimate its causal effect on political outcomes. In this paper, I develop a theoretical framework that explains how changes to election timing affect the electoral fortunes of organized interest groups. I test the theory examining the effects of a 2006 Texas law that forced approximately 20 percent of the state’s school by districts to move their elections to the same day as national elections. ## Loading Data load(&quot;Data/Texas_school_board.RData&quot;) 4.10 Write in notation and estimate the pooled model of the effect of OnCycle on LnAvgSalary. The model is: \\[LnAvgSalary_{it} = \\beta_0 + \\beta_1OnCycle_{it} + \\epsilon_{it}\\] ## Pooled model dta %&gt;% lm(LnAvgSalary ~ OnCycle, .) %&gt;% summary() ## ## Call: ## lm(formula = LnAvgSalary ~ OnCycle, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.28910 -0.05527 -0.00646 0.05196 0.65215 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.671363 0.001018 10478.815 &lt;2e-16 *** ## OnCycle -0.030621 0.003766 -8.131 5e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.08284 on 7137 degrees of freedom ## (64 observations deleted due to missingness) ## Multiple R-squared: 0.009178, Adjusted R-squared: 0.009039 ## F-statistic: 66.11 on 1 and 7137 DF, p-value: 4.995e-16 4.11 Write in notation and estimate a difference-in-difference model of the effect of OnCycle on LnAvgSalary. What is the key coefficient that we are interested in? \\[LnAvgSalary_{it} = \\beta_0 + \\beta_1CycleSwitch_{i} + \\beta_2After_{t} + \\beta_3(CycleSwitch_{i}\\times After_{t}) + \\epsilon_{it}\\] ## DiD model dta %&gt;% lm(LnAvgSalary ~ CycleSwitch*AfterSwitch, .) %&gt;% summary() ## ## Call: ## lm(formula = LnAvgSalary ~ CycleSwitch * AfterSwitch, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.29811 -0.05595 -0.00734 0.05284 0.65245 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.671068 0.001433 7447.598 &lt; 2e-16 *** ## CycleSwitch -0.023982 0.003397 -7.059 1.83e-12 *** ## AfterSwitch 0.009303 0.002188 4.251 2.16e-05 *** ## CycleSwitch:AfterSwitch -0.008590 0.005189 -1.655 0.0979 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.08334 on 7198 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.0183, Adjusted R-squared: 0.01789 ## F-statistic: 44.72 on 3 and 7198 DF, p-value: &lt; 2.2e-16 "],["lab-iv-project-runway.html", "5 2/13 Lab IV | Project Runway 5.1 Base R Version 5.2 library(ggplot) Version 1 5.3 library(ggplot) Version 2 5.4 Alternative Plot of Your Choice 5.5 Code Appendix", " 5 2/13 Lab IV | Project Runway Our goal is to visualize the difference between the population percent (popPct) and the survey percent (svyPct) for various age groups. We’ll use the data in the table below (and of course, a full viz would include more subgroups). Create designs on how to present this information. Be ready to share concept and actual viz with the entire class. You can either work individually or in small groups. Do not include code with your visualizations. Instead, create an appendix that displays each code chunk at the end of the document. Make sure there are no warnings or messages displaying too. Use the simulated data to make at least two plots: one in Base R and one in library(ggplot). Then you can use a dataset of your choice for the last two visualizations or keep working with the fake data. Table 5.1: Table: Population and Survey Percentages by Age Group age popPct svyPct 18 to 29 29 19 36 to 50 21 21 51 to 64 30 32 65+ 20 28 5.1 Base R Version 5.2 library(ggplot) Version 1 5.3 library(ggplot) Version 2 5.4 Alternative Plot of Your Choice 5.5 Code Appendix 5.5.1 Setup Code # Load packages used in this session of R library(knitr) library(tidyverse) library(ggplot2) opts_chunk$set(echo = TRUE) options(digits = 2) 5.5.2 Preparation Code df &lt;- data.frame(&quot;age&quot; = c(&quot;18 to 29&quot;, &quot;36 to 50&quot;, &quot;51 to 64&quot;, &quot;65+&quot;), &quot;popPct&quot; = c(29, 21, 30, 20), &quot;svyPct&quot; = c(19, 21, 32, 28)) kable(df, caption = &quot;Table: Population and survey percentages by age group&quot;) 5.5.3 Base R Plot Code Age18to29 &lt;- c(19, 29) Age36to50 &lt;- c(21,21) Age51to64 &lt;- c(32, 30) Over65 &lt;- c(28, 20) age_groups &lt;- cbind(Age18to29, Age36to50, Age51to64, Over65) barplot(age_groups, beside=T, xlab=&quot;Age Group&quot;, names.arg= c(&quot;18 - 29&quot;, &quot;36 - 50&quot;, &quot;51 - 64&quot;, &quot;65+&quot;), ylab=&quot;Percent&quot;, main = &quot;Percent Surveyed and Percent in Population by Age Group&quot;, ylim = c(0,35), las=1) legend(&quot;bottomleft&quot;,c(&quot;Surveyed %&quot;, &quot;Population %&quot;), fill=c(&quot;black&quot;, &quot;light gray&quot;), horiz=FALSE, cex=0.73, bg=&quot;white&quot;) 5.5.4 library(ggplot) First Plot Code df %&gt;% mutate(Population = popPct, Survey = svyPct) %&gt;% dplyr::select(-popPct, -svyPct) %&gt;% pivot_longer(-age, names_to=&quot;Group&quot;, values_to=&quot;Percent&quot;) %&gt;% ggplot(aes(x=age, y=Percent, fill=Group)) + geom_bar(stat=&quot;identity&quot;, position=&quot;dodge&quot;) + scale_fill_grey() + theme_minimal() + labs(x = &quot;Age Group&quot;, y = &quot;Percent&quot;, title = &quot;Population and Survey Sample Proportions by Age Group&quot;) 5.5.5 library(ggplot) Second Plot Code df %&gt;% mutate(Population = popPct, Survey = svyPct) %&gt;% dplyr::select(-popPct, -svyPct) %&gt;% pivot_longer(-age, names_to=&quot;Group&quot;, values_to=&quot;Percent&quot;) %&gt;% ggplot(aes(x=age, y=Percent, fill=Group)) + geom_bar(stat=&quot;identity&quot;, position=&quot;dodge&quot;) + coord_flip() + scale_fill_grey() + theme_minimal() + labs(x = &quot;Age Group&quot;, y = &quot;Percent&quot;, title = &quot;Population and Survey Sample Proportions by Age Group&quot;) 5.5.6 Alternative Plot Code library(apyramid) df %&gt;% mutate(Population = popPct, Survey = svyPct) %&gt;% dplyr::select(-popPct, -svyPct) %&gt;% pivot_longer(-age, names_to=&quot;Group&quot;, values_to=&quot;Percent&quot;) %&gt;% mutate(age = as.factor(age)) %&gt;% age_pyramid(data = ., age_group = &quot;age&quot;, split_by = &quot;Group&quot;, count = &quot;Percent&quot;, show_midpoint = FALSE) + scale_fill_grey() + theme_minimal() + labs(x=&quot;Age Group&quot;, y=&quot;Percent&quot;, fill=NULL, title = &quot;Percent Surveyed and Percent in Population by Age Group&quot;) "],["lab-v-2sls-instrumental-variables.html", "6 2/21 Lab V | 2SLS &amp; Instrumental Variables 6.1 Preparation 6.2 Estimate a basic OLS model with “Do others regard you as religious” as the dependent variable as a function of Hajj2006. Explain how there might be endogeneity. 6.3 State the requirements of a good instrument and xplain how the “success” variable may satisfy these conditions for a good instrumental variable. 6.4 Use two different packages to estimate a 2SLS model of Religious as a function of Hajj2006. You can use library(AER), library(ivreg), library(fixest) or any other package for estimating 2SLS models. 6.5 Show the first stage from the 2SLS model above with lm(). Explain the implications of the results. 6.6 Add covariates for age, literacy, urban, group size and gender to the 2SLS model Religious as a function of Hajj2006. What is different? Which variables are included in the first stage? 6.7 Run multiple 2SLS models with OssamaIncorrect, GovtForce, NatlInterest, Happy, GirlsSchool and JobsWomen variables as dependent variables. Use the list of covariates from earlier. If you want, try using a loop (but not necessary).", " 6 2/21 Lab V | 2SLS &amp; Instrumental Variables 6.1 Preparation ## Packages library(haven) ## Package to read Stata data library(ivreg) ## Package to run 2sls library(fixest) ## This package can also run 2SLS library(tidyverse) ## For tidyverse commands library(memisc) ## For table outputting ## Loading Data hajj_public &lt;- read_dta(&quot;Data/hajj_public.dta&quot;) Do important life experiences influence political and social views? In particular, does performing the Hajj pilgrimage to Mecca affect the views of pilgrims? David Clingingsmith, Asim Ijaz Khwaja, and Michael Kremer (2009) analyze this question by using two-stage least squares to compare successful and unsuccessful applicants in a lottery used by Pakistan to allocate Hajj visas. We will conduct pared-down models. The paper creates indices and implements additional statistical procedures to produce a broader and clearer picture. It is not a bad idea to read this paper to see how we can extend the methods we learn in class to your own work. I posted the paper on Canvas for your convenience. Data description Variable Description hajj2006 Went on Hajj trip in 2006 success Won the lottery to have expenses covered for Hajj ptygrp Categorical variable indicating size of party for Hajj trip smallpty 1 if small party group, 0 otherwise urban 1 if live in urban area, 0 otherwise age Age female 1 if female, 0 otherwise literate 1 if literate, 0 otherwise x_s7q10 Natl affairs: How often do you follow national affairs in the news on television or on the radio? Binary: 0=Twice a week or less, 1=Several times a week or more x_s14aq10 Religious: Do others regard you as religious? Binary: 1=Religious, 0=Not Religious x_s10bq4 OssamaIncorrect: Do you believe goals Ossama is fighting for are correct? Binary: 1=Not Correct at All/Slightly Incorrect, 0=Correct/Absolutely Correct x_s7q12a GovtForce: Govt should force people to conform to Islamic injunctions. Binary: 1=Agree Strongly/Agree, 0=Neutral/Disagree/Strongly Disagree x_s7q1 NatlInterest How interested would you say you are in national affairs? Binary: 0=Not interested, 1=Interested x_s3q3 Happy: how happy are you? From 1 (not at all happy) to 4 (very happy). x_s10eq2 GirlsSchool: In your opinion, girls should attend school. Binary: 0=Disagree, 1=Agree s10dq1 JobsWomen: When jobs are scarce, men should always have more right to a job than women. Binary: 0=Generally agree, 1=Generally Disagree More details on these and other variables are available in Appendix 3 of the paper. If you cannot access the version, the SSRN version works as well. 6.2 Estimate a basic OLS model with “Do others regard you as religious” as the dependent variable as a function of Hajj2006. Explain how there might be endogeneity. hajj_public %&gt;% lm(x_s14aq10~hajj2006, data=.) %&gt;% broom::tidy() ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.767 0.0154 49.8 5 e-323 ## 2 hajj2006 0.0851 0.0199 4.27 2.09e- 5 There may be endogeneity due to baseline bias caused by the religiosity of respondents. A more religious respondent may be more likely to go on a Hajj trip and be classified as religious by others. Going to church, like actually being religious, is also a factor that may be correlated with x and lurking in the error term. 6.3 State the requirements of a good instrument and xplain how the “success” variable may satisfy these conditions for a good instrumental variable. The two conditions, inclusion and exclusion, are: \\[Cov(X,Z)\\ne0\\] &amp; \\[Cov(Z,\\epsilon)=0\\] The lottery is randomizes, which means it is not correlated with the error term, or anything else other than the treatment variable, in our model. Further, it meaningfully effects our key independent which is tested below. 6.4 Use two different packages to estimate a 2SLS model of Religious as a function of Hajj2006. You can use library(AER), library(ivreg), library(fixest) or any other package for estimating 2SLS models. ## With library(ivreg) hajj_public %&gt;% ivreg(x_s14aq10~hajj2006 | success, data=.) %&gt;% broom::tidy() ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.757 0.0169 44.9 1.09e-281 ## 2 hajj2006 0.101 0.0231 4.40 1.18e- 5 ## With library(fixest) hajj_public %&gt;% feols(x_s14aq10 ~ 1 | hajj2006~success, data=., vcov = &quot;iid&quot;) %&gt;% broom::tidy() ## NOTE: 64 observations removed because of NA values (LHS: 64). ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.757 0.0169 44.9 1.09e-281 ## 2 fit_hajj2006 0.101 0.0231 4.40 1.18e- 5 6.5 Show the first stage from the 2SLS model above with lm(). Explain the implications of the results. hajj_public %&gt;% lm(hajj2006~success, data=.) %&gt;% broom::tidy() ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.137 0.00893 15.4 6.35e-50 ## 2 success 0.854 0.0122 69.9 0 The t-score is 69.8663143 which is much higher than the 3 threshold. Our instrument meets in the inclusion condition. 6.6 Add covariates for age, literacy, urban, group size and gender to the 2SLS model Religious as a function of Hajj2006. What is different? Which variables are included in the first stage? hajj_public %&gt;% feols(x_s14aq10~age + literate + ptygrp + female + urban | hajj2006 ~ success + age + literate + ptygrp + female + urban, data=., vcov = &quot;iid&quot;) %&gt;% broom::tidy() ## NOTE: 64 observations removed because of NA values (LHS: 64). ## The instruments &#39;age&#39;, &#39;literate&#39; and 3 others have been removed because of collinearity (see $collin.var). ## # A tibble: 7 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.493 0.0623 7.92 4.41e-15 ## 2 fit_hajj2006 0.101 0.0227 4.46 8.71e- 6 ## 3 age 0.00269 0.000800 3.36 7.97e- 4 ## 4 literate 0.0146 0.0238 0.616 5.38e- 1 ## 5 ptygrp -0.000955 0.00642 -0.149 8.82e- 1 ## 6 female 0.132 0.0217 6.06 1.69e- 9 ## 7 urban 0.0674 0.0213 3.17 1.56e- 3 hajj_public %&gt;% ivreg(x_s14aq10~hajj2006 + age + literate + ptygrp + female + urban | success + age + literate + ptygrp + female + urban, data=.) %&gt;% summary(diagnostics=TRUE) ## ## Call: ## ivreg(formula = x_s14aq10 ~ hajj2006 + age + literate + ptygrp + ## female + urban | success + age + literate + ptygrp + female + ## urban, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.9971 0.0486 0.1494 0.2186 0.4310 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.4934864 0.0622823 7.923 4.41e-15 *** ## hajj2006 0.1012173 0.0226841 4.462 8.71e-06 *** ## age 0.0026874 0.0007997 3.361 0.000797 *** ## literate 0.0146344 0.0237657 0.616 0.538131 ## ptygrp -0.0009545 0.0064199 -0.149 0.881821 ## female 0.1316934 0.0217261 6.062 1.69e-09 *** ## urban 0.0673979 0.0212624 3.170 0.001556 ** ## ## Diagnostic tests: ## df1 df2 statistic p-value ## Weak instruments 1 1534 4594.635 &lt;2e-16 *** ## Wu-Hausman 1 1533 1.787 0.182 ## Sargan 0 NA NA NA ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3776 on 1534 degrees of freedom ## Multiple R-Squared: 0.04825, Adjusted R-squared: 0.04453 ## Wald test: 13.18 on 6 and 1534 DF, p-value: 1.349e-14 6.7 Run multiple 2SLS models with OssamaIncorrect, GovtForce, NatlInterest, Happy, GirlsSchool and JobsWomen variables as dependent variables. Use the list of covariates from earlier. If you want, try using a loop (but not necessary). ## OssamaIncorrect hajj_public %&gt;% ivreg(x_s10bq4~hajj2006 + age + literate + ptygrp + female + urban | success + age + literate + ptygrp + female + urban, data=.) %&gt;% broom::tidy() ## GovtForce hajj_public %&gt;% ivreg(x_s7q12a~hajj2006 + age + literate + ptygrp + female + urban | success + age + literate + ptygrp + female + urban, data=.) %&gt;% broom::tidy() ## NatlInterest hajj_public %&gt;% ivreg(x_s7q1~hajj2006 + age + literate + ptygrp + female + urban | success + age + literate + ptygrp + female + urban, data=.) %&gt;% broom::tidy() ## Happy hajj_public %&gt;% ivreg(x_s3q3~hajj2006 + age + literate + ptygrp + female + urban | success + age + literate + ptygrp + female + urban, data=.) %&gt;% broom::tidy() ## Girl School hajj_public %&gt;% ivreg(x_s10eq2~hajj2006 + age + literate + ptygrp + female + urban | success + age + literate + ptygrp + female, data=.) %&gt;% broom::tidy() ## Jobs Women hajj_public %&gt;% ivreg(x_s10dq1~hajj2006 + age + literate + ptygrp + female + urban | success + age + literate + ptygrp + female, data=.) %&gt;% broom::tidy() ## Loop ## DVs dvs &lt;- c(&quot;hajj_public$x_s10bq4&quot;, &quot;hajj_public$x_s7q12a&quot;, &quot;hajj_public$x_s7q1&quot;, &quot;hajj_public$x_s3q3&quot;, &quot;hajj_public$x_s10eq2&quot;, &quot;hajj_public$x_s10dq1&quot;) ## Loop for(i in 1:length(dvs)){ model &lt;- paste(&quot;model&quot;,i, sep=&quot;&quot;) m &lt;- ivreg(as.formula(paste(dvs[i],&quot;~hajj2006 + age + literate + ptygrp + female + urban | success + age + literate + ptygrp + female + urban&quot;)), data=hajj_public) assign(model,m)} "],["lab-vi-experiments.html", "7 3/13 Lab VI | Experiments 7.1 Preparation 7.2 Why is an experiment useful in this context? 7.3 Is there evidence of balance in the treatment? Why is this relevant? Check for balance of each treatment with respect to age category, gender, education, income, voting for Sisi and not voting for non-boycott reasons. Use two different procedures to conduct your balance tests. You do not need to use TOST! 7.4 What is the effect of the treatments on whether respondents viewed the police tactics as justified? Run one model with only the treatment variables and another that includes controls for age category, gender, education, income, voting for Sisi and not voting for non-boycott reasons. 7.5 Test whether the effects of treatment 1 (police-favorable message) are the same as treatment 3 (both sides) for each dependent variable. Feel free to use the linearHypothesis function in the car package. 7.6 In some political contexts, messages have a different effect on people with less engagement in politics. Are there heterogeneous treatment effects by those who did not vote for reasons other than to boycott the election and those who did not? Feel free to explore multiple other ways there could be heterogeneous effects (e.g., perhaps there are different effects on Sisi supporters), but you only need to report on possible differential effects of the treatments on the “not-accountable” dependent variable.", " 7 3/13 Lab VI | Experiments 7.1 Preparation library(knitr) library(haven) library(tidyverse) library(car) library(memisc) load(&quot;~/GOVT702/Data/EgyptProtests.RData&quot;) Williamson and Malik (2020) used a survey experiment to examine the public response to how Egyptians responded to different framings regarding a protest in which protesters were killed by police. All respondents were given the following statement: “Last year, police raided an apartment in 6 October City. During the raid, they killed 9 members of the Muslim Brotherhood.” Respondents were assigned with equal probability to a control group that got no more information or one of three treatment groups that received additional information. Respondents in the first treatment group were provided a paragraph that justified the killings from the security forces perspective. Respondents in the second treatment group were provided a paragraph that criticized the killings from a human rights perspective. Respondents in the third treatment group were provided both the security forces and human rights perspectives. Two outcomes were measured: if respondents thought the police tactics were justified, and if the police should the police be held accountable for killing the nine men. So that both variables are coded in same direction, this variable is coded as 1 if the respondent gave a pro-police answer (saying the police did not need to be held accountable). Data description Variable Description just_binary_main 1 if the respondent said the police tactics were justified; 0 otherwise. not_acc_binary_main 1 if the respondent said the police should definitely or probably not be held accountable; 0 otherwise. We call this variable no-accountability treat_b1 1 if respondent was given the pro-police paragraph treat_b2 1 if respondent was given the human rights paragraph treat_b3 1 if respondent was given both the pro-police and human rights paragraphs sisi_vote 1 if the respondent voted for Sisi not_voteboycott 1 if the respondent did not vote for some other reason than boycotting the election opposition_vote_boycott 1 if the respondent either voted for the opposition candidate or boycotted the election education from 1 (no formal education) to 8 (graduate degree). Values from 6 to 8 went to college. income 1 = Less than 500 pounds per month, 2 = 500 to 1000 pounds per month, 3 = 1000 to 4000 pounds per month, 4 = 4000 to 10000 pounds per month, 5 = More than 10000 pounds per month male 1 for men, 0 otherwise age_cat age categories: 1 is less than 35, 2 is 35 to 50 and 3 is over 50. Published paper (paywalled) Draft version of the paper 7.2 Why is an experiment useful in this context? Simply asking respondents if they agree or disagree with the police does not answer if and how different messages work. If respondents were simply asked if they agree with the either the pro-police or human rights messages it may be difficult to separate out whether the message was persuasive or whether the respondent was predisposed to approve of the argument. 7.3 Is there evidence of balance in the treatment? Why is this relevant? Check for balance of each treatment with respect to age category, gender, education, income, voting for Sisi and not voting for non-boycott reasons. Use two different procedures to conduct your balance tests. You do not need to use TOST! ## Removing NAs dta &lt;- dta[is.na(dta$just_binary_main)==0 &amp; is.na(dta$not_acc_binary_main)==0,] dep_vars &lt;- dta %&gt;% dplyr::select(age_cat, male, education, sisi_vote, not_voteboycott, income) b_test &lt;- lapply(dep_vars, function(bal) { lm(bal ~ dta$treat_b1 + dta$treat_b2 + dta$treat_b3)}) ## Extracting Slopes and P Values slopes &lt;- round(sapply(b_test, function(x) x$coefficients), 2) p &lt;- round(sapply(b_test, function(x) { summary(x)$coefficients[,4]}), 2) ## Showing Results balance_results = data.frame(b1= slopes[2,], b1.p = p[2,], b2= slopes[3,], b2.p = p[3,], b3= slopes[4,], b3.p = p[4,]) balance_results ## b1 b1.p b2 b2.p b3 b3.p ## age_cat 0.05 0.60 0.01 0.89 -0.01 0.89 ## male 0.01 0.77 0.00 0.90 0.05 0.18 ## education 0.45 0.01 0.27 0.12 0.04 0.82 ## sisi_vote 0.02 0.74 0.05 0.36 0.09 0.12 ## not_voteboycott -0.02 0.71 -0.02 0.76 -0.05 0.37 ## income 0.18 0.10 0.06 0.55 0.24 0.03 We can do the same with a loop. ## DV Creation dvs &lt;- c(&quot;dta$age_cat&quot;, &quot;dta$male&quot;, &quot;dta$education&quot;, &quot;dta$income&quot;, &quot;dta$sisi_vote&quot;, &quot;dta$not_voteboycott&quot;) ## Loop for Treatments for(i in 1:length(dvs)){ model &lt;- paste(&quot;model&quot;,i, sep=&quot;&quot;) m &lt;- lm(as.formula(paste(dvs[i],&quot;~treat_b1 + treat_b2 + treat_b3&quot;)), data=dta) assign(model,m)} mtable(model1, model2, model3, model4, model5, model6) There is some evidence of imbalance. 7.4 What is the effect of the treatments on whether respondents viewed the police tactics as justified? Run one model with only the treatment variables and another that includes controls for age category, gender, education, income, voting for Sisi and not voting for non-boycott reasons. ## Without Controls dta %&gt;% lm(just_binary_main~treat_b1 + treat_b2 + treat_b3, data=.) %&gt;% summary() ## ## Call: ## lm(formula = just_binary_main ~ treat_b1 + treat_b2 + treat_b3, ## data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.4589 -0.3453 -0.3154 0.5411 0.6846 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.31544 0.03923 8.042 5.02e-15 *** ## treat_b1 0.14347 0.05576 2.573 0.0103 * ## treat_b2 0.01342 0.05547 0.242 0.8089 ## treat_b3 0.02989 0.05646 0.529 0.5968 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4788 on 579 degrees of freedom ## Multiple R-squared: 0.01409, Adjusted R-squared: 0.008977 ## F-statistic: 2.757 on 3 and 579 DF, p-value: 0.04167 ## With Controls dta %&gt;% lm(just_binary_main~treat_b1 + treat_b2 + treat_b3 + factor(age_cat) + male + education + income + sisi_vote + not_voteboycott, data=.) %&gt;% summary() ## ## Call: ## lm(formula = just_binary_main ~ treat_b1 + treat_b2 + treat_b3 + ## factor(age_cat) + male + education + income + sisi_vote + ## not_voteboycott, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.7901 -0.1992 -0.1011 0.3328 0.9448 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.0771076 0.1015603 0.759 0.4481 ## treat_b1 0.1189739 0.0499768 2.381 0.0176 * ## treat_b2 -0.0249841 0.0496725 -0.503 0.6152 ## treat_b3 -0.0169674 0.0506942 -0.335 0.7380 ## factor(age_cat)2 -0.0362495 0.0425449 -0.852 0.3946 ## factor(age_cat)3 0.0005895 0.0471999 0.012 0.9900 ## male 0.0494751 0.0534350 0.926 0.3549 ## education -0.0007622 0.0125688 -0.061 0.9517 ## income -0.0010071 0.0209086 -0.048 0.9616 ## sisi_voteTRUE 0.5494024 0.0414417 13.257 &lt;2e-16 *** ## not_voteboycottTRUE 0.0806941 0.0453807 1.778 0.0760 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4036 on 516 degrees of freedom ## (56 observations deleted due to missingness) ## Multiple R-squared: 0.2926, Adjusted R-squared: 0.2789 ## F-statistic: 21.34 on 10 and 516 DF, p-value: &lt; 2.2e-16 7.5 Test whether the effects of treatment 1 (police-favorable message) are the same as treatment 3 (both sides) for each dependent variable. Feel free to use the linearHypothesis function in the car package. reg.1 &lt;- lm(just_binary_main~treat_b1 + treat_b2 + treat_b3 + factor(age_cat) + male + education + income + sisi_vote + not_voteboycott, data=dta) linearHypothesis(reg.1, &quot;treat_b1=treat_b3&quot;) ## Linear hypothesis test ## ## Hypothesis: ## treat_b1 - treat_b3 = 0 ## ## Model 1: restricted model ## Model 2: just_binary_main ~ treat_b1 + treat_b2 + treat_b3 + factor(age_cat) + ## male + education + income + sisi_vote + not_voteboycott ## ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 517 85.236 ## 2 516 84.062 1 1.1742 7.2075 0.007494 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 7.6 In some political contexts, messages have a different effect on people with less engagement in politics. Are there heterogeneous treatment effects by those who did not vote for reasons other than to boycott the election and those who did not? Feel free to explore multiple other ways there could be heterogeneous effects (e.g., perhaps there are different effects on Sisi supporters), but you only need to report on possible differential effects of the treatments on the “not-accountable” dependent variable. dta %&gt;% lm(not_acc_binary_main ~ not_voteboycott*treat_b1 + not_voteboycott*treat_b2 + not_voteboycott*treat_b3 +factor(age_cat) + male + education + income + sisi_vote, data = .) %&gt;% summary() ## ## Call: ## lm(formula = not_acc_binary_main ~ not_voteboycott * treat_b1 + ## not_voteboycott * treat_b2 + not_voteboycott * treat_b3 + ## factor(age_cat) + male + education + income + sisi_vote, ## data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.58236 -0.22916 -0.05162 0.03384 0.99812 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.022268 0.095838 0.232 0.81636 ## not_voteboycottTRUE 0.250014 0.075859 3.296 0.00105 ** ## treat_b1 0.105574 0.053546 1.972 0.04919 * ## treat_b2 0.038517 0.053542 0.719 0.47223 ## treat_b3 0.144074 0.054048 2.666 0.00793 ** ## factor(age_cat)2 0.009428 0.039361 0.240 0.81078 ## factor(age_cat)3 0.015627 0.043724 0.357 0.72094 ## male -0.017692 0.049679 -0.356 0.72189 ## education -0.007508 0.011635 -0.645 0.51902 ## income 0.006283 0.019388 0.324 0.74602 ## sisi_voteTRUE 0.434241 0.038357 11.321 &lt; 2e-16 *** ## not_voteboycottTRUE:treat_b1 -0.091467 0.105260 -0.869 0.38527 ## not_voteboycottTRUE:treat_b2 -0.212318 0.104539 -2.031 0.04277 * ## not_voteboycottTRUE:treat_b3 -0.337778 0.108394 -3.116 0.00193 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3733 on 513 degrees of freedom ## (56 observations deleted due to missingness) ## Multiple R-squared: 0.2461, Adjusted R-squared: 0.2269 ## F-statistic: 12.88 on 13 and 513 DF, p-value: &lt; 2.2e-16 "],["lab-vii-lists-loops-functions-in-r.html", "8 3/20 Lab VII | Lists, Loops, &amp; Functions in R 8.1 Preparation 8.2 Load data. Stock price data is in stocks2020.csv. Presidential prices are in USPres_2020_Price History By Market -Bulk.xlsx. Think about renaming Data, Contract Name and Close Share Price for ease of use. 8.3 Create data frame with daily price data for Trump in 2020 8.4 Merge the stock price and Trump price data 8.5 Create a list of stock ticker names (do not include DJI as a ticker) 8.6 Create a function that calculates daily percent change. Use the lag function and the mutate_at function. Make sure to check that your function worked. 8.7 Loop thru list of tickers and run regressions in which daily change in stock price is a function of change in DJIA and change in Trump price. Create a data frame that stores the coefficient, standard error and t-stat for the Trump variable for each stock. Include a column that has the stock ticker in that data frame as well. 8.8 The questions below are not required for full credit but I encourage you to try!", " 8 3/20 Lab VII | Lists, Loops, &amp; Functions in R 8.1 Preparation ## Packages library(plyr) library(tidyverse) library(readxl) library(readr) library(lubridate) library(knitr) Differences in policies and personnel associated with presidential candidates could affect the profitability of firms in different ways, with stocks potentially rising and falling with the expected outlooks for political candidates. In this report we analyze stocks that have been identified as potentially responsive to the political fortunes of Donald Trump and Joe Biden. 8.1.1 Data Our stock market data consists of daily closing prices of 31 stocks that were identified by Kiplinger as being politically exposed. These stocks cover a broad range of sectors potentially influenced by politics, including tech stocks, energy (both oil and renewable oriented firms), health care, marijuana, gold and foreign-oriented ETFs. Our measure of political expectations comes from the PredictIt betting market in which investors buy shares in (among other things) presidential candidates. Share are worth $1 if the candidate wins the general election. The price for a candidate on a given day is taken to be the market’s estimate of the probability of victory for that candidate. 8.2 Load data. Stock price data is in stocks2020.csv. Presidential prices are in USPres_2020_Price History By Market -Bulk.xlsx. Think about renaming Data, Contract Name and Close Share Price for ease of use. ## Reading in stock data stk &lt;- read_csv(&quot;Data/stocks2020.csv&quot;, col_names = TRUE) ## Reading in presidential market data pres_mkt &lt;- read_xlsx(&quot;Data/USPres_2020_Price History By Market -Bulk.xlsx&quot;, col_names = TRUE) %&gt;% mutate(date = `Date (ET)`, name = `Contract Name`, price = `Close Share Price`) %&gt;% dplyr::select(-`Date (ET)`, -`Contract Name`, -`Close Share Price`) 8.3 Create data frame with daily price data for Trump in 2020 ## Creating daily price data for Trump trump2020 &lt;- pres_mkt %&gt;% filter(year(date) &gt; 2019 &amp; name == &quot;Donald Trump&quot;) %&gt;% dplyr::select(c(date, &quot;Trump&quot; = price)) 8.4 Merge the stock price and Trump price data ## Joining the two datasets stk_pres &lt;- left_join(stk, trump2020, by = &quot;date&quot;) 8.5 Create a list of stock ticker names (do not include DJI as a ticker) ## Creating list of ticker names stk_tickers &lt;- c(names(stk)[!names(stk) %in% c(&quot;date&quot;, &quot;DJI&quot;)]) ## Creating number of stocks stk_num &lt;- length(stk_tickers) 8.6 Create a function that calculates daily percent change. Use the lag function and the mutate_at function. Make sure to check that your function worked. ## Price Change Function pct_change &lt;- function(x){ (x - lag(x))/lag(x) } ## Daily Change Dataset daily_df &lt;- stk_pres %&gt;% mutate_at(c(&quot;DJI&quot;, &quot;Trump&quot;, stk_tickers), pct_change) %&gt;% dplyr::select(c(date, c(&quot;DJI&quot;, &quot;Trump&quot;, stk_tickers))) 8.7 Loop thru list of tickers and run regressions in which daily change in stock price is a function of change in DJIA and change in Trump price. Create a data frame that stores the coefficient, standard error and t-stat for the Trump variable for each stock. Include a column that has the stock ticker in that data frame as well. ## Preparing Data Frame to store results ols_results &lt;- data.frame(&quot;row&quot; = 1:stk_num, &quot;ticker&quot; = NA, &quot;coef&quot; = NA, &quot;se&quot; = NA, &quot;tStat&quot; = NA) ## Looping Though Regressions for(i in 1:stk_num){ daily_df$temp &lt;- unlist(daily_df[, i + 3]) ols.1 &lt;- lm(temp ~ DJI + Trump, data = daily_df) ols_results[i, &quot;ticker&quot;] &lt;- stk_tickers[i] ols_results[i, 3:5] &lt;- round(summary(ols.1)$coefficients[&quot;Trump&quot;, 1:3], 3) } ## Showing results head(ols_results) ## row ticker coef se tStat ## 1 1 AAPL -0.015 0.011 -1.304 ## 2 2 NFLX -0.036 0.016 -2.208 ## 3 3 AMZN -0.034 0.013 -2.643 ## 4 4 K -0.003 0.011 -0.259 ## 5 5 TSLA -0.004 0.031 -0.115 ## 6 6 LMT 0.000 0.011 -0.026 8.8 The questions below are not required for full credit but I encourage you to try! 8.8.1 Use list apply - lapply() - to regress stock price on Trump. ## Now with lapply() ols_results2 &lt;- lapply(stk_tickers, function(x){ daily_df$temp = unlist(daily_df[, which(names(daily_df) == x)]) ols.1 &lt;- lm(temp ~ DJI + Trump, data = daily_df) }) ## Showing Results head(ols_results2) ## [[1]] ## ## Call: ## lm(formula = temp ~ DJI + Trump, data = daily_df) ## ## Coefficients: ## (Intercept) DJI Trump ## 0.001889 0.980104 -0.014758 ## ## ## [[2]] ## ## Call: ## lm(formula = temp ~ DJI + Trump, data = daily_df) ## ## Coefficients: ## (Intercept) DJI Trump ## 0.001903 0.535397 -0.035819 ## ## ## [[3]] ## ## Call: ## lm(formula = temp ~ DJI + Trump, data = daily_df) ## ## Coefficients: ## (Intercept) DJI Trump ## 0.002097 0.532408 -0.033624 ## ## ## [[4]] ## ## Call: ## lm(formula = temp ~ DJI + Trump, data = daily_df) ## ## Coefficients: ## (Intercept) DJI Trump ## -0.0003296 0.4133016 -0.0028461 ## ## ## [[5]] ## ## Call: ## lm(formula = temp ~ DJI + Trump, data = daily_df) ## ## Coefficients: ## (Intercept) DJI Trump ## 0.009543 1.042136 -0.003596 ## ## ## [[6]] ## ## Call: ## lm(formula = temp ~ DJI + Trump, data = daily_df) ## ## Coefficients: ## (Intercept) DJI Trump ## -0.0002786 0.8337098 -0.0002856 8.8.2 Use tapply to find the average price of each presidential candidate over the course of the campaign. ## With tapply() tapply(pres_mkt$price, pres_mkt$name, mean) ## Amy Klobuchar Andrew Cuomo Andrew Yang Bernie Sanders Beto O&#39;Rourke Cory Booker ## 0.02166512 0.01606446 0.03927786 0.09528414 0.03062162 0.03407125 ## Donald Trump Elizabeth Warren Hillary Clinton Howie Hawkins Jo Jorgensen Joe Biden ## 0.38840543 0.07018660 0.02375566 0.01000000 0.01000000 0.24708227 ## John Kasich Kamala Harris Kanye West Kirsten Gillibrand Mark Cuban Mark Zuckerberg ## 0.01776930 0.06757422 0.01000000 0.03126378 0.01106383 0.01024052 ## Michael Bloomberg Mike Pence Nikki Haley Paul Ryan Pete Buttigieg Sherrod Brown ## 0.02249647 0.03799830 0.01691027 0.01322849 0.04516535 0.01289140 ## Tom Steyer Tulsi Gabbard ## 0.01000000 0.01221106 8.8.3 Now use map() in library(purrr) to regress stock price on Trump. ## The function from above func_stock &lt;- function(x){ daily_df$temp = unlist(daily_df[, which(names(daily_df) == x)]) ols.1 &lt;- lm(temp ~ DJI + Trump, data = daily_df) } ## Now with map() map(.x=stk_tickers, .f=func_stock) ## [[1]] ## ## Call: ## lm(formula = temp ~ DJI + Trump, data = daily_df) ## ## Coefficients: ## (Intercept) DJI Trump ## 0.001889 0.980104 -0.014758 ## ## ## [[2]] ## ## Call: ## lm(formula = temp ~ DJI + Trump, data = daily_df) ## ## Coefficients: ## (Intercept) DJI Trump ## 0.001903 0.535397 -0.035819 ## ## ## [[3]] ## ## Call: ## lm(formula = temp ~ DJI + Trump, data = daily_df) ## ## Coefficients: ## (Intercept) DJI Trump ## 0.002097 0.532408 -0.033624 ## ## ## [[4]] ## ## Call: ## lm(formula = temp ~ DJI + Trump, data = daily_df) ## ## Coefficients: ## (Intercept) DJI Trump ## -0.0003296 0.4133016 -0.0028461 ## ## ## [[5]] ## ## Call: ## lm(formula = temp ~ DJI + Trump, data = daily_df) ## ## Coefficients: ## (Intercept) DJI Trump ## 0.009543 1.042136 -0.003596 ## ## ## [[6]] ## ## Call: ## lm(formula = temp ~ DJI + Trump, data = daily_df) ## ## Coefficients: ## (Intercept) DJI Trump ## -0.0002786 0.8337098 -0.0002856 ## ## ## [[7]] ## ## Call: ## lm(formula = temp ~ DJI + Trump, data = daily_df) ## ## Coefficients: ## (Intercept) DJI Trump ## 0.0003462 0.9118574 -0.0070218 ## ## ## [[8]] ## ## Call: ## lm(formula = temp ~ DJI + Trump, data = daily_df) ## ## Coefficients: ## (Intercept) DJI Trump ## -0.0007476 1.3508059 0.0489526 ## ## ## [[9]] ## ## Call: ## lm(formula = temp ~ DJI + Trump, data = daily_df) ## ## Coefficients: ## (Intercept) DJI Trump ## 0.001328 0.658680 -0.008615 ## ## ## [[10]] ## ## Call: ## lm(formula = temp ~ DJI + Trump, data = daily_df) ## ## Coefficients: ## (Intercept) DJI Trump ## -0.002651 1.056977 0.028651 ## ## ## [[11]] ## ## Call: ## lm(formula = temp ~ DJI + Trump, data = daily_df) ## ## Coefficients: ## (Intercept) DJI Trump ## 0.001382 1.011709 -0.010030 ## ## ## [[12]] ## ## Call: ## lm(formula = temp ~ DJI + Trump, data = daily_df) ## ## Coefficients: ## (Intercept) DJI Trump ## 0.00125 0.86138 -0.04332 ## ## ## [[13]] ## ## Call: ## lm(formula = temp ~ DJI + Trump, data = daily_df) ## ## Coefficients: ## (Intercept) DJI Trump ## 0.0008178 0.8435450 -0.0202986 ## ## ## [[14]] ## ## Call: ## lm(formula = temp ~ DJI + Trump, data = daily_df) ## ## Coefficients: ## (Intercept) DJI Trump ## -0.003142 1.067967 0.019232 ## ## ## [[15]] ## ## Call: ## lm(formula = temp ~ DJI + Trump, data = daily_df) ## ## Coefficients: ## (Intercept) DJI Trump ## 0.001017 0.463254 -0.002714 ## ## ## [[16]] ## ## Call: ## lm(formula = temp ~ DJI + Trump, data = daily_df) ## ## Coefficients: ## (Intercept) DJI Trump ## -0.002939 0.866421 0.005825 ## ## ## [[17]] ## ## Call: ## lm(formula = temp ~ DJI + Trump, data = daily_df) ## ## Coefficients: ## (Intercept) DJI Trump ## 0.0001886 1.0378297 0.0003729 ## ## ## [[18]] ## ## Call: ## lm(formula = temp ~ DJI + Trump, data = daily_df) ## ## Coefficients: ## (Intercept) DJI Trump ## 4.747e-05 1.067e+00 2.402e-02 ## ## ## [[19]] ## ## Call: ## lm(formula = temp ~ DJI + Trump, data = daily_df) ## ## Coefficients: ## (Intercept) DJI Trump ## 0.0005725 0.9488423 0.0269447 ## ## ## [[20]] ## ## Call: ## lm(formula = temp ~ DJI + Trump, data = daily_df) ## ## Coefficients: ## (Intercept) DJI Trump ## -0.0005572 0.5019176 -0.0012461 ## ## ## [[21]] ## ## Call: ## lm(formula = temp ~ DJI + Trump, data = daily_df) ## ## Coefficients: ## (Intercept) DJI Trump ## 0.001302 1.009206 0.004588 ## ## ## [[22]] ## ## Call: ## lm(formula = temp ~ DJI + Trump, data = daily_df) ## ## Coefficients: ## (Intercept) DJI Trump ## 0.0006483 0.8554046 0.0061128 ## ## ## [[23]] ## ## Call: ## lm(formula = temp ~ DJI + Trump, data = daily_df) ## ## Coefficients: ## (Intercept) DJI Trump ## 0.001798 0.893112 -0.010831 ## ## ## [[24]] ## ## Call: ## lm(formula = temp ~ DJI + Trump, data = daily_df) ## ## Coefficients: ## (Intercept) DJI Trump ## 0.001051 0.810440 -0.008493 ## ## ## [[25]] ## ## Call: ## lm(formula = temp ~ DJI + Trump, data = daily_df) ## ## Coefficients: ## (Intercept) DJI Trump ## 0.002559 0.937766 0.005098 ## ## ## [[26]] ## ## Call: ## lm(formula = temp ~ DJI + Trump, data = daily_df) ## ## Coefficients: ## (Intercept) DJI Trump ## 0.01192 0.69831 0.05660 ## ## ## [[27]] ## ## Call: ## lm(formula = temp ~ DJI + Trump, data = daily_df) ## ## Coefficients: ## (Intercept) DJI Trump ## 0.0008981 1.0917988 -0.0242325 ## ## ## [[28]] ## ## Call: ## lm(formula = temp ~ DJI + Trump, data = daily_df) ## ## Coefficients: ## (Intercept) DJI Trump ## -0.001052 1.176215 0.019945 ## ## ## [[29]] ## ## Call: ## lm(formula = temp ~ DJI + Trump, data = daily_df) ## ## Coefficients: ## (Intercept) DJI Trump ## -0.002581 0.959133 0.031636 ## ## ## [[30]] ## ## Call: ## lm(formula = temp ~ DJI + Trump, data = daily_df) ## ## Coefficients: ## (Intercept) DJI Trump ## 0.0001306 0.8001097 -0.0106032 ## ## ## [[31]] ## ## Call: ## lm(formula = temp ~ DJI + Trump, data = daily_df) ## ## Coefficients: ## (Intercept) DJI Trump ## 0.0008454 0.0707473 -0.0124007 8.8.4 Create your own simple regression function. To avoid issues of NAs, simulate some data and see if it worked! ## Creating constituent functions ## Beta Hat Function beta_hat &lt;- function(x,y) { (sum((x-mean(x))*(y-mean(y))))/ (sum((x-mean(x))^2)) } ## Intercept Function constant &lt;- function(x,y,b){ mean(y) - b*mean(x) } ## Fitted Values Function fit_val &lt;- function(x, y, b) { constant(x=x,y=y,b=b) + beta_hat(x,y) * x } ## R squared Function r2 &lt;- function(fitted, y) { sum((fitted - mean(y))^2)/ sum((y-mean(y))^2) } ## Standard Error Function se &lt;- function(fitted, x, y) { sqrt(sum((fitted-y)^2)/length(y)/ sum((x-mean(x))^2)) } ## T Stat Function t &lt;- function(beta, null=0, se) { (beta-null)/ se } ## P-Value Function pval &lt;- function(t,df) { pt(q=t, df, lower.tail = F)*2 } ## OLS Function ols &lt;- function(X,Y) { b &lt;- beta_hat(X, Y) constant &lt;- constant(x=X, y=Y, b) fitted &lt;- fit_val(x=X,y=Y,b=b) r_squared &lt;- r2(fitted, y=Y) stand_err &lt;- se(fitted=fitted, x=X, y=Y) t_stat &lt;- t(beta=b, se=stand_err) p &lt;- pval(t=t_stat, df=(length(X)-1)) tibble(b, stand_err, t_stat, p) } ## Creating Data x &lt;- rnorm(1000, 15, 2) y &lt;- x*17 + rnorm(1000, 15, 12) ## Testing ols(x, y) ## # A tibble: 1 × 4 ## b stand_err t_stat p ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 17.1 0.198 86.5 0 summary(lm(y~x)) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -37.736 -7.979 -0.183 7.687 45.240 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 13.2274 3.0075 4.398 1.21e-05 *** ## x 17.1354 0.1983 86.427 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 12.16 on 998 degrees of freedom ## Multiple R-squared: 0.8821, Adjusted R-squared: 0.882 ## F-statistic: 7470 on 1 and 998 DF, p-value: &lt; 2.2e-16 "],["lab-viii-regression-discontinuity-designs.html", "9 3/27 Lab VIII | Regression Discontinuity Designs 9.1 Preparation 9.2 Run the most basic RDD model and then replicate the main result using the specification for Table 1. In this model, the RD allows for differential quadratic models above and below the threshold and also has dummy variables for birthdays. The data is limited to observations within 2 years of 21. Do this with lm(). Finally, run a diagnostic plot to test for sorting just beyond the cutoff. 9.3 Run the basic rd model and the model with covariates with library(rdrobust). What is different? 9.4 Estimate a series of models with lm() that have slightly different specifications. First, try the above specification with only linear age variable, first fixing slope to be the same above and below the threshhold and then allowing slope to vary above and below the threshhold. 9.5 Now see what happens when you use a different window sizes. Feel free to experiment (but only need to report one specification.) Just include the treatment and running variables and allow the slopes to differ. 9.6 Now create some RDD figures. I suggest using rdplot() in library(rdrobust) and experimenting with different arguments.", " 9 3/27 Lab VIII | Regression Discontinuity Designs 9.1 Preparation ## Packages library(knitr) library(haven) library(tidyverse) library(rdrobust) # Load data: data saved as object named &quot;dta&quot; load(&quot;~/GOVT702/Data/Ch11_Lab_AlcoholCrime.RData&quot;) Carpenter and Dobkin (2015) analyzes the relationship between alcohol and crime using a regression discontinuity design. In this lab we will replicate the results and then explore different specifications. As you will see, some reasonable alternative specifications yield quite different results. The goal here will be to see if we can make any progress in explaining why the results vary as we vary the specification. From the codebook: “The individual level arrest records are collapsed into arrest counts by age in days for each crime type in the SAS program”P01 Estimate Age Profile of Crime Rates.sas”. This program uses populations estimated from the census in “P00 Estimate Population Denominators.sas” to compute arrest rates per 10,000.” The main variables are all_r: number of arrests across all categories. Our main dependent variable. Arrest data is also broken down for violent_r, property_r, ill_drugs_r and alcohol_r post a dumm variable indicating the individual is over 21 years of age linear: the number of years from being age 21 (that is, linear = 1.0 indicates 22 year olds. Negative values means the individual is under 21 and positive values indicate the individual is over than 21. square: the squared value of linear linear_post: interaction of linear and post square_post: interaction of square and post birthday dummies: e.g., birthday_19 is a dummy for a person’s 19th birthday and birthday_19_1 is a dummy for the day after a person’s 19th birthday. Use this code in your formula: + birthday_19 + birthday_19_1 + birthday_20 + birthday_20_1 + birthday_21 + birthday_21_1 + birthday_22 + birthday_22_1 + birthday_23 + birthday_23_1 See Carpenter, Christopher and Carlos Dobkin. 2015. The Minimum Legal Drinking Age and Crime. The Review of Economics and Statistics. 97:2, 521-524. 9.2 Run the most basic RDD model and then replicate the main result using the specification for Table 1. In this model, the RD allows for differential quadratic models above and below the threshold and also has dummy variables for birthdays. The data is limited to observations within 2 years of 21. Do this with lm(). Finally, run a diagnostic plot to test for sorting just beyond the cutoff. ## Basic RD model rd_basic &lt;- lm(all_r ~ post + linear, data=dta[abs(dta$linear) &lt;= 2, ]) summary(rd_basic) ## ## Call: ## lm(formula = all_r ~ post + linear, data = dta[abs(dta$linear) &lt;= ## 2, ]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -89.90 -22.34 -2.25 16.94 695.67 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1534.914 2.537 605.00 &lt;2e-16 *** ## post 76.893 4.536 16.95 &lt;2e-16 *** ## linear -48.792 1.963 -24.86 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 43.34 on 1458 degrees of freedom ## Multiple R-squared: 0.3249, Adjusted R-squared: 0.324 ## F-statistic: 350.9 on 2 and 1458 DF, p-value: &lt; 2.2e-16 ## Differing Slopes rd_basic &lt;- lm(all_r ~ post*linear, data=dta[abs(dta$linear) &lt;= 2, ]) summary(rd_basic) ## ## Call: ## lm(formula = all_r ~ post * linear, data = dta[abs(dta$linear) &lt;= ## 2, ]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -89.83 -19.48 -2.83 14.90 671.76 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1559.027 3.043 512.287 &lt;2e-16 *** ## post 76.762 4.298 17.860 &lt;2e-16 *** ## linear -24.712 2.633 -9.386 &lt;2e-16 *** ## post:linear -48.061 3.720 -12.921 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 41.07 on 1457 degrees of freedom ## Multiple R-squared: 0.3943, Adjusted R-squared: 0.3931 ## F-statistic: 316.2 on 3 and 1457 DF, p-value: &lt; 2.2e-16 ## Model with covariates and vary rd_replicate &lt;- lm(all_r ~ post + linear + square + linear_post + square_post + birthday_19 + birthday_20 + birthday_20_1 + birthday_21 + birthday_21_1 + birthday_22 + birthday_22_1, data=dta[abs(dta$linear) &lt;= 2, ]) summary(rd_replicate) ## ## Call: ## lm(formula = all_r ~ post + linear + square + linear_post + square_post + ## birthday_19 + birthday_20 + birthday_20_1 + birthday_21 + ## birthday_21_1 + birthday_22 + birthday_22_1, data = dta[abs(dta$linear) &lt;= ## 2, ]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -83.005 -17.601 -0.801 15.893 216.107 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1543.068 2.924 527.744 &lt; 2e-16 *** ## post 91.214 4.146 21.998 &lt; 2e-16 *** ## linear -71.056 6.760 -10.511 &lt; 2e-16 *** ## square -23.726 3.274 -7.248 6.86e-13 *** ## linear_post -16.259 9.573 -1.698 0.0896 . ## square_post 33.666 4.629 7.272 5.77e-13 *** ## birthday_19 287.420 26.396 10.889 &lt; 2e-16 *** ## birthday_20 408.587 26.275 15.551 &lt; 2e-16 *** ## birthday_20_1 217.604 26.275 8.282 2.73e-16 *** ## birthday_21 633.829 26.398 24.010 &lt; 2e-16 *** ## birthday_21_1 673.303 26.396 25.508 &lt; 2e-16 *** ## birthday_22 347.872 26.275 13.240 &lt; 2e-16 *** ## birthday_22_1 382.068 26.275 14.541 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 26.23 on 1448 degrees of freedom ## Multiple R-squared: 0.7544, Adjusted R-squared: 0.7524 ## F-statistic: 370.6 on 12 and 1448 DF, p-value: &lt; 2.2e-16 ## Diagnostic plot hist(dta$linear, main = &quot;Diagnostic Plot&quot;, xlab = &quot;Age&quot;, las=1) 9.3 Run the basic rd model and the model with covariates with library(rdrobust). What is different? ## With RD robust rd_1 &lt;- rdrobust(x = dta$linear, y = dta$all_r) summary(rd_1) ## Sharp RD estimates using local polynomial regression. ## ## Number of Obs. 2922 ## BW type mserd ## Kernel Triangular ## VCE method NN ## ## Number of Obs. 1461 1461 ## Eff. Number of Obs. 164 165 ## Order est. (p) 1 1 ## Order bias (q) 2 2 ## BW est. (h) 0.451 0.451 ## BW bias (b) 0.957 0.957 ## rho (h/b) 0.471 0.471 ## Unique Obs. 1461 1461 ## ## ============================================================================= ## Method Coef. Std. Err. z P&gt;|z| [ 95% C.I. ] ## ============================================================================= ## Conventional 138.387 21.391 6.469 0.000 [96.462 , 180.313] ## Robust - - 6.124 0.000 [98.077 , 190.409] ## ============================================================================= summary(rdrobust(x = dta$linear, y = dta$all_r, covs = dta$birthday_19 + dta$birthday_20 + dta$birthday_20_1 + dta$birthday_21 + dta$birthday_21_1 + dta$birthday_22 + dta$birthday_22_1, p=1)) ## Covariate-adjusted Sharp RD estimates using local polynomial regression. ## ## Number of Obs. 2922 ## BW type mserd ## Kernel Triangular ## VCE method NN ## ## Number of Obs. 1461 1461 ## Eff. Number of Obs. 160 161 ## Order est. (p) 1 1 ## Order bias (q) 2 2 ## BW est. (h) 0.440 0.440 ## BW bias (b) 0.811 0.811 ## rho (h/b) 0.542 0.542 ## Unique Obs. 1461 1461 ## ## ============================================================================= ## Method Coef. Std. Err. z P&gt;|z| [ 95% C.I. ] ## ============================================================================= ## Conventional 93.650 6.282 14.908 0.000 [81.338 , 105.962] ## Robust - - 13.500 0.000 [81.902 , 109.722] ## ============================================================================= 9.4 Estimate a series of models with lm() that have slightly different specifications. First, try the above specification with only linear age variable, first fixing slope to be the same above and below the threshhold and then allowing slope to vary above and below the threshhold. ## Additional Models RD.1 &lt;- lm(all_r ~ post + linear + birthday_19 + birthday_20 + birthday_20_1 + birthday_21 + birthday_21_1 + birthday_22 + birthday_22_1, data=dta[abs(dta$linear) &lt;= 2, ]) summary(RD.1) ## ## Call: ## lm(formula = all_r ~ post + linear + birthday_19 + birthday_20 + ## birthday_20_1 + birthday_21 + birthday_21_1 + birthday_22 + ## birthday_22_1, data = dta[abs(dta$linear) &lt;= 2, ]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -86.028 -20.191 -0.524 18.625 200.851 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1536.997 1.736 885.501 &lt; 2e-16 *** ## post 68.655 3.108 22.089 &lt; 2e-16 *** ## linear -45.492 1.344 -33.836 &lt; 2e-16 *** ## birthday_19 249.713 29.650 8.422 &lt; 2e-16 *** ## birthday_20 416.496 29.620 14.062 &lt; 2e-16 *** ## birthday_20_1 225.573 29.620 7.616 4.7e-14 *** ## birthday_21 662.459 29.650 22.342 &lt; 2e-16 *** ## birthday_21_1 701.818 29.650 23.670 &lt; 2e-16 *** ## birthday_22 344.618 29.620 11.635 &lt; 2e-16 *** ## birthday_22_1 378.755 29.620 12.787 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 29.6 on 1451 degrees of freedom ## Multiple R-squared: 0.6867, Adjusted R-squared: 0.6848 ## F-statistic: 353.4 on 9 and 1451 DF, p-value: &lt; 2.2e-16 RD.2 &lt;- lm(all_r ~ post + linear + linear_post + birthday_19 + birthday_20 + birthday_20_1 + birthday_21 + birthday_21_1 + birthday_22 + birthday_22_1, data=dta[abs(dta$linear) &lt;= 2, ]) summary(RD.2) ## ## Call: ## lm(formula = all_r ~ post + linear + linear_post + birthday_19 + ## birthday_20 + birthday_20_1 + birthday_21 + birthday_21_1 + ## birthday_22 + birthday_22_1, data = dta[abs(dta$linear) &lt;= ## 2, ]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -85.700 -17.549 -0.791 15.896 222.679 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1558.886 1.986 785.091 &lt;2e-16 *** ## post 68.715 2.811 24.445 &lt;2e-16 *** ## linear -23.604 1.720 -13.726 &lt;2e-16 *** ## linear_post -43.777 2.432 -18.002 &lt;2e-16 *** ## birthday_19 271.602 26.842 10.118 &lt;2e-16 *** ## birthday_20 416.496 26.787 15.548 &lt;2e-16 *** ## birthday_20_1 225.513 26.787 8.419 &lt;2e-16 *** ## birthday_21 640.510 26.843 23.862 &lt;2e-16 *** ## birthday_21_1 679.930 26.842 25.330 &lt;2e-16 *** ## birthday_22 344.558 26.787 12.863 &lt;2e-16 *** ## birthday_22_1 378.755 26.787 14.139 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 26.77 on 1450 degrees of freedom ## Multiple R-squared: 0.7439, Adjusted R-squared: 0.7422 ## F-statistic: 421.2 on 10 and 1450 DF, p-value: &lt; 2.2e-16 9.5 Now see what happens when you use a different window sizes. Feel free to experiment (but only need to report one specification.) Just include the treatment and running variables and allow the slopes to differ. ## Different window sizes RD.3 &lt;- lm(all_r ~ post + linear + linear_post, data=dta[abs(dta$linear) &lt;= 1, ]) summary(RD.3) ## ## Call: ## lm(formula = all_r ~ post + linear + linear_post, data = dta[abs(dta$linear) &lt;= ## 1, ]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -85.20 -21.41 -3.22 15.40 665.30 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1543.846 4.973 310.460 &lt; 2e-16 *** ## post 98.439 7.013 14.036 &lt; 2e-16 *** ## linear -53.881 8.595 -6.269 6.24e-10 *** ## linear_post -33.659 12.131 -2.775 0.00567 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 47.4 on 727 degrees of freedom ## Multiple R-squared: 0.2209, Adjusted R-squared: 0.2176 ## F-statistic: 68.69 on 3 and 727 DF, p-value: &lt; 2.2e-16 RD.4 &lt;- lm(all_r ~ post + linear + linear_post, data=dta[abs(dta$linear) &lt;= 3, ]) summary(RD.4) ## ## Call: ## lm(formula = all_r ~ post + linear + linear_post, data = dta[abs(dta$linear) &lt;= ## 3, ]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -256.54 -26.50 1.32 30.71 671.72 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1629.580 3.635 448.252 &lt;2e-16 *** ## post 6.250 5.137 1.217 0.224 ## linear 54.926 2.097 26.187 &lt;2e-16 *** ## linear_post -127.401 2.964 -42.979 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 60.11 on 2187 degrees of freedom ## Multiple R-squared: 0.4705, Adjusted R-squared: 0.4698 ## F-statistic: 647.9 on 3 and 2187 DF, p-value: &lt; 2.2e-16 9.6 Now create some RDD figures. I suggest using rdplot() in library(rdrobust) and experimenting with different arguments. ## With rdrobust rdplot(dta$all_r , dta$linear, c=0, kernel = &quot;triangular&quot;, x.label = &quot;Year from Age Cutoff&quot;, y.label = &quot;Arrest Rate&quot;, title = &quot;Effect of Alcohol on Crime&quot;, binselect = &quot;es&quot;) # Create a dataframe that contains average crime rates grouped by bin, limited to the # window set by the window parameter dta$age_fortnight = 21 + (14*floor(dta$days_to_21/14))/365 # Create a variable placing each observation in a &quot;bin&quot; dta$bin = rep(1:length(unique(dta$age_fortnight)), table(dta$age_fortnight)) window &lt;- 4 dta_bin &lt;- dta %&gt;% select(days_to_21, all_r, property_r, age_fortnight, alcohol_r, bin) %&gt;% group_by(bin) %&gt;% summarise(property_r_bin = mean(property_r), all_r_bin = mean(all_r), alcohol_r_bin = mean(alcohol_r), age_fortnight_bin = mean(age_fortnight)) %&gt;% filter(age_fortnight_bin &gt;= 21- window &amp; age_fortnight_bin &lt;= 21 + window) # Linear model with varying slopes below and above threshhold # ALCOHOL ARREST RATES rd_linear_left &lt;- lm(all_r ~ linear, dta[dta$linear &gt; -window &amp; dta$linear &lt;0, ]) rd_linear_right &lt;- lm(all_r ~ linear, dta[dta$linear &lt; window &amp; dta$linear &gt;0, ]) # Create a scatter plot plot(dta_bin$age_fortnight_bin, dta_bin$all_r_bin, type = &quot;p&quot;, pch = 1, cex = 0.5, cex.main = 0.8, xlab = &quot;&quot;, ylab = &quot;&quot;, xaxt=&#39;n&#39;, yaxt=&#39;n&#39;) axis(2, las = 1, tick = T, cex.axis = .8, mgp = c(2,.7,0)) axis(1, tick = T, at= seq(17, 25, by=1), labels =seq(17, 25, by=1),cex.axis = .8, mgp = c(2,.3,0)) mtext(&quot;Arrest rate&quot;, las = 1, side = 2, at = 520, line = -0.2, cex = 1) mtext(&quot;Age at time of arrest&quot;, side = 1, line = 1., cex = 0.8) # Add fitted lines from &quot;left&quot; and &quot;right&quot; models points(seq(21 - window, 21, by = 0.1), coef(rd_linear_left) [1] + coef(rd_linear_left) [2]*seq(-window, 0, by = 0.1), lwd = 2, col = &quot;darkblue&quot;, type = &#39;l&#39;) points(seq(21, 21 + window, by = 0.1), coef(rd_linear_right)[1] + coef(rd_linear_right)[2]*seq(0, window , by = 0.1), lwd = 2, col = &quot;darkblue&quot;, type = &#39;l&#39;) "],["lab-ix-time-series.html", "10 4/13 Lab IX | Time Series 10.1 Estimate a model with the log of miles driven (use the unadjusted data: logMilesNA) as a function of log gas prices, unemployment and log of population. (Don’t account for autocorrelation.) Is the effect of gas statistically significant? 10.2 Do you think there is seasonality in driving? Test by adding dummy variables for month to the above model. Is there evidence of monthly variation? Is the effect of gas statistically significant? Now create a categorical variable for seasons and run the model without the month dummy variable. 10.3 Briefly explain why there might be autocorrelation. 10.4 Create a figure that is useful to assess whether there is autocorrelation (you have two choices here). Draw a sketch here. 10.5 Test whether there is first order autocorrelation. Report the key statistic from this test. What would a coefficient of 1 indicate? 10.6 Use Newey-West standard errors to account for autocorrelation. Use the same variables as in part (b). Determine the t-stats and describe the similarities and difference with earlier results. 10.7 Estimate a model that adjusts for autocorrelation. Use the same variables as in part (b). Describe similarities and difference with earlier results. 10.8 Estimate a dynamic model of miles driven using control variables from above. Discuss key differences. 10.9 Run an Augmented Dickey-Fuller Test on logMilesNA. 10.10 OPTIONAL QUESTION: Conduct the same analysis from a, b, d, and g using light truck sales as the dependent variable. (Light truck sales use more gas than cars, so the question is whether gas prices affect the kind of car people buy which will affect gas consumption for the life of the car.)", " 10 4/13 Lab IX | Time Series ## Packages library(haven) library(AER) library(orcutt) library(tidyverse) library(lubridate) library(lmtest) library(sandwich) ## Loading Data load(&quot;~/GOVT702/Data/Ch13_Lab_GasPrices.RData&quot;) Data description: Variable Description year Year month Month time Time identifier (1 for first observation, etc) logMilesNA Log of vehicle miles traveled (in millions) in the U.S., not seasonally adjusted logLightTruckSales Log of retail sales of light weight trucks in the U.S. (in thousands) logGasReal Log of real price of gas unem Unemployment rate logPop Log of U.S. population (in thousands) 10.1 Estimate a model with the log of miles driven (use the unadjusted data: logMilesNA) as a function of log gas prices, unemployment and log of population. (Don’t account for autocorrelation.) Is the effect of gas statistically significant? ## Model 1 reg.1 &lt;- dta %&gt;% lm(logMilesNA ~ logGasReal + unem + logPop, data = .) ## Summary Output summary(reg.1) ## ## Call: ## lm(formula = logMilesNA ~ logGasReal + unem + logPop, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.195230 -0.045729 0.004914 0.051965 0.136480 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -5.918752 1.065393 -5.555 6.11e-08 *** ## logGasReal 0.030388 0.021152 1.437 0.152 ## unem -0.019785 0.002654 -7.455 9.67e-13 *** ## logPop 1.461736 0.084586 17.281 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.06833 on 300 degrees of freedom ## (260 observations deleted due to missingness) ## Multiple R-squared: 0.7243, Adjusted R-squared: 0.7215 ## F-statistic: 262.7 on 3 and 300 DF, p-value: &lt; 2.2e-16 ## 95% CI confint(reg.1) ## 2.5 % 97.5 % ## (Intercept) -8.01534205 -3.82216115 ## logGasReal -0.01123688 0.07201218 ## unem -0.02500777 -0.01456290 ## logPop 1.29527941 1.62819344 10.2 Do you think there is seasonality in driving? Test by adding dummy variables for month to the above model. Is there evidence of monthly variation? Is the effect of gas statistically significant? Now create a categorical variable for seasons and run the model without the month dummy variable. ## Removing NAs dta &lt;- dta %&gt;% drop_na() ## Creating a variable for seasons dta &lt;- dta %&gt;% mutate(season = case_when( month == 1 | month == 2 | month == 3 ~ &quot;Winter&quot;, month == 4 | month == 5 | month == 6 ~ &quot;Spring&quot;, month == 7 | month == 8 | month == 9 ~ &quot;Summer&quot;, month == 10 | month == 11 | month == 12 ~ &quot;Fall&quot;) ) ## Model 2 Months reg.2a &lt;- dta %&gt;% lm(logMilesNA ~ logGasReal + unem + logPop + factor(month), data = .) ## Summary Output summary(reg.2a) ## ## Call: ## lm(formula = logMilesNA ~ logGasReal + unem + logPop + factor(month), ## data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.078372 -0.017092 0.000947 0.019921 0.058033 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -8.432427 0.443617 -19.008 &lt; 2e-16 *** ## logGasReal -0.042114 0.008961 -4.700 4.04e-06 *** ## unem -0.014137 0.001122 -12.603 &lt; 2e-16 *** ## logPop 1.651863 0.035166 46.974 &lt; 2e-16 *** ## factor(month)2 -0.047461 0.007907 -6.003 5.81e-09 *** ## factor(month)3 0.099815 0.007933 12.583 &lt; 2e-16 *** ## factor(month)4 0.093377 0.008030 11.628 &lt; 2e-16 *** ## factor(month)5 0.142644 0.008063 17.690 &lt; 2e-16 *** ## factor(month)6 0.144017 0.008003 17.995 &lt; 2e-16 *** ## factor(month)7 0.169429 0.007981 21.228 &lt; 2e-16 *** ## factor(month)8 0.165200 0.008007 20.633 &lt; 2e-16 *** ## factor(month)9 0.074265 0.007976 9.311 &lt; 2e-16 *** ## factor(month)10 0.113230 0.007960 14.224 &lt; 2e-16 *** ## factor(month)11 0.042180 0.007915 5.329 1.99e-07 *** ## factor(month)12 0.050981 0.007893 6.459 4.45e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.02795 on 289 degrees of freedom ## Multiple R-squared: 0.9556, Adjusted R-squared: 0.9534 ## F-statistic: 444 on 14 and 289 DF, p-value: &lt; 2.2e-16 ## 95% CI confint(reg.2a) ## 2.5 % 97.5 % ## (Intercept) -9.30555634 -7.55929713 ## logGasReal -0.05975083 -0.02447708 ## unem -0.01634434 -0.01192881 ## logPop 1.58264989 1.72107617 ## factor(month)2 -0.06302371 -0.03189915 ## factor(month)3 0.08420205 0.11542848 ## factor(month)4 0.07757182 0.10918316 ## factor(month)5 0.12677323 0.15851447 ## factor(month)6 0.12826530 0.15976855 ## factor(month)7 0.15372049 0.18513846 ## factor(month)8 0.14944155 0.18095910 ## factor(month)9 0.05856676 0.08996317 ## factor(month)10 0.09756222 0.12889798 ## factor(month)11 0.02660157 0.05775803 ## factor(month)12 0.03544623 0.06651523 ## Model 2 Seasons reg.2b &lt;- dta %&gt;% lm(logMilesNA ~ logGasReal + unem + logPop + factor(season), data = .) ## Summary Output summary(reg.2b) ## ## Call: ## lm(formula = logMilesNA ~ logGasReal + unem + logPop + factor(season), ## data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.139222 -0.040786 0.004486 0.035948 0.140897 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -7.832131 0.806242 -9.714 &lt; 2e-16 *** ## logGasReal -0.026933 0.016238 -1.659 0.0983 . ## unem -0.014513 0.002033 -7.137 7.33e-12 *** ## logPop 1.609829 0.063964 25.168 &lt; 2e-16 *** ## factor(season)Spring 0.056839 0.008335 6.819 5.12e-11 *** ## factor(season)Summer 0.065784 0.008295 7.931 4.46e-14 *** ## factor(season)Winter -0.050893 0.008424 -6.042 4.56e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.05104 on 297 degrees of freedom ## Multiple R-squared: 0.8477, Adjusted R-squared: 0.8446 ## F-statistic: 275.5 on 6 and 297 DF, p-value: &lt; 2.2e-16 ## 95% CI confint(reg.2b) ## 2.5 % 97.5 % ## (Intercept) -9.41880203 -6.245459970 ## logGasReal -0.05888954 0.005024539 ## unem -0.01851452 -0.010511204 ## logPop 1.48394893 1.735709462 ## factor(season)Spring 0.04043612 0.073242797 ## factor(season)Summer 0.04946019 0.082107980 ## factor(season)Winter -0.06747086 -0.034315628 10.3 Briefly explain why there might be autocorrelation. With 50 years of data, there are certainly time trends and we can reasonably expect the error term of one year to be correlated with the error term of the previous year. For example, if more people began longer commutes in the 1990s, the longer commutes (more miles driven) will continue into the 2000s so the errors will be correlated year by year. 10.4 Create a figure that is useful to assess whether there is autocorrelation (you have two choices here). Draw a sketch here. ## Saving the Residuals dta$residual_errors &lt;- resid(reg.2a) ## Regressing Residuals lag_err &lt;- c(NA, dta$residual_errors[1:(length(dta$residual_errors)-1)]) plot(lag_err, dta$residual_errors, pch=20, xlab = &quot;Lagged Residuals&quot;, ylab = &quot;Residuals&quot;, bty=&quot;n&quot;, col=&quot;dark blue&quot;, las=1) abline(lm(dta$residual_errors ~ lag_err), lwd=2) ## Plotting Over Time plot(dta$residual_errors, pch=20, xlab = &quot;Time&quot;, las=1, ylab = &quot;Residuals&quot;, bty=&quot;n&quot;, col=&quot;dark blue&quot;) # Creating season category and plotting residual error dta %&gt;% ggplot() + geom_line(aes(x=as.Date(date), y=residual_errors, col=season), size=.8) + scale_color_manual(values=c(&quot;orange&quot;, &quot;green&quot;, &quot;red&quot;, &quot;darkblue&quot;))+ labs(x=&quot;Date&quot;, y=&quot;Residual Errors&quot;) + theme_classic() + theme(panel.grid.major.y=element_line(color=&quot;darkgrey&quot;, linetype=&quot;dotted&quot;), panel.grid.minor.y=element_line(color=&quot;darkgrey&quot;, linetype=&quot;dotted&quot;)) %&gt;% labs(col=&quot;Season&quot;) # Creating season category and plotting residual error dta %&gt;% ggplot() + geom_point(aes(x=as.Date(date), y=residual_errors, col=season)) + scale_color_manual(values=c(&quot;orange&quot;, &quot;green&quot;, &quot;red&quot;, &quot;darkblue&quot;))+ labs(x=&quot;Date&quot;, y=&quot;Residual Errors&quot;) + theme_classic() + theme(panel.grid.major.y=element_line(color=&quot;darkgrey&quot;, linetype=&quot;dotted&quot;), panel.grid.minor.y=element_line(color=&quot;darkgrey&quot;, linetype=&quot;dotted&quot;)) %&gt;% labs(col=&quot;Season&quot;) 10.5 Test whether there is first order autocorrelation. Report the key statistic from this test. What would a coefficient of 1 indicate? ## Error on Lagged Error Regression reg.lag &lt;- lm(dta$residual_errors ~ lag_err) summary(reg.lag) ## ## Call: ## lm(formula = dta$residual_errors ~ lag_err) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.074188 -0.010223 0.001141 0.010386 0.065748 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -4.056e-05 8.993e-04 -0.045 0.964 ## lag_err 8.215e-01 3.300e-02 24.892 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.01565 on 301 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.673, Adjusted R-squared: 0.6719 ## F-statistic: 619.6 on 1 and 301 DF, p-value: &lt; 2.2e-16 ## Durbin-Watson dwtest(reg.2a) ## ## Durbin-Watson test ## ## data: reg.2a ## DW = 0.35861, p-value &lt; 2.2e-16 ## alternative hypothesis: true autocorrelation is greater than 0 10.6 Use Newey-West standard errors to account for autocorrelation. Use the same variables as in part (b). Determine the t-stats and describe the similarities and difference with earlier results. ## Calculating Newey West Standard Errors sqrt(diag(NeweyWest(reg.2a, lag=4, prewhite=FALSE, adjust=TRUE))) ## (Intercept) logGasReal unem logPop factor(month)2 factor(month)3 factor(month)4 factor(month)5 ## 1.038631354 0.017795927 0.001853709 0.082605874 0.005951753 0.006987587 0.008119800 0.008298968 ## factor(month)6 factor(month)7 factor(month)8 factor(month)9 factor(month)10 factor(month)11 factor(month)12 ## 0.008732203 0.008721058 0.008926142 0.008676850 0.007994856 0.007639009 0.006481667 ## Checking t-stats coef(reg.2a) / sqrt(diag(NeweyWest(reg.2a, lag=4, prewhite=FALSE, adjust=TRUE))) ## (Intercept) logGasReal unem logPop factor(month)2 factor(month)3 factor(month)4 factor(month)5 ## -8.118787 -2.366494 -7.626101 19.996920 -7.974362 14.284654 11.499973 17.188144 ## factor(month)6 factor(month)7 factor(month)8 factor(month)9 factor(month)10 factor(month)11 factor(month)12 ## 16.492622 19.427629 18.507471 8.558977 14.162868 5.521632 7.865373 10.7 Estimate a model that adjusts for autocorrelation. Use the same variables as in part (b). Describe similarities and difference with earlier results. ## Cochrane-Orcutt Model reg.orc &lt;- cochrane.orcutt(reg.2a) summary(reg.orc) ## Call: ## lm(formula = logMilesNA ~ logGasReal + unem + logPop + factor(month), ## data = .) ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -8.5660946 1.1602976 -7.383 1.675e-12 *** ## logGasReal -0.0545711 0.0165253 -3.302 0.0010800 ** ## unem -0.0106665 0.0030232 -3.528 0.0004867 *** ## logPop 1.6604913 0.0922935 17.991 &lt; 2.2e-16 *** ## factor(month)2 -0.0467377 0.0033259 -14.053 &lt; 2.2e-16 *** ## factor(month)3 0.1018515 0.0046115 22.087 &lt; 2.2e-16 *** ## factor(month)4 0.0977499 0.0059402 16.456 &lt; 2.2e-16 *** ## factor(month)5 0.1473488 0.0064157 22.967 &lt; 2.2e-16 *** ## factor(month)6 0.1473300 0.0062046 23.745 &lt; 2.2e-16 *** ## factor(month)7 0.1725746 0.0061698 27.971 &lt; 2.2e-16 *** ## factor(month)8 0.1692116 0.0062837 26.929 &lt; 2.2e-16 *** ## factor(month)9 0.0794105 0.0063119 12.581 &lt; 2.2e-16 *** ## factor(month)10 0.1184317 0.0060001 19.738 &lt; 2.2e-16 *** ## factor(month)11 0.0466867 0.0052127 8.956 &lt; 2.2e-16 *** ## factor(month)12 0.0549369 0.0042181 13.024 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.0157 on 298 degrees of freedom ## Multiple R-squared: 0.9368 , Adjusted R-squared: 0.936 ## F-statistic: 305.1 on 4 and 298 DF, p-value: &lt; 1.79e-163 ## ## Durbin-Watson statistic ## (original): 0.35861 , p-value: 5.127e-46 ## (transformed): 2.58848 , p-value: 1e+00 ## &quot;By Hand&quot; reg.2a &lt;- dta %&gt;% lm(logMilesNA ~ logGasReal + unem + logPop + factor(month), data = .) ## Error on Lagged Error Regression reg.lag &lt;- lm(dta$residual_errors ~ lag_err) rho &lt;- summary(reg.lag)$coef[2] ## Lagging Each Variable dta$lagmile &lt;- dplyr::lag(dta$logMilesNA, ordered_by = dta$year) dta$lag_gas &lt;- dplyr::lag(dta$logGasReal, ordered_by = dta$year) dta$lag_unem &lt;- dplyr::lag(dta$unem, ordered_by = dta$year) dta$lag_pop &lt;- dplyr::lag(dta$logPop, ordered_by = dta$year) dta$lag_month &lt;- dplyr::lag(dta$month, ordered_by = dta$year) ## Rho-transforming rho_mile &lt;- dta$logMilesNA - rho*dta$lagmile rho_gas &lt;- dta$logGasReal - rho*dta$lag_gas rho_unem &lt;- dta$unem - rho*dta$lag_unem rho_pop &lt;- dta$logPop - rho*dta$lag_pop rho_month &lt;- dta$month - rho*dta$lag_month ## Running rho-transformed model rho_trans_model &lt;- lm(rho_mile ~ rho_gas + rho_unem + rho_pop + factor(rho_month)) summary(rho_trans_model) ## ## Call: ## lm(formula = rho_mile ~ rho_gas + rho_unem + rho_pop + factor(rho_month)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.074031 -0.010478 0.001731 0.010546 0.065795 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.574882 0.200142 -7.869 7.29e-14 *** ## rho_gas -0.054194 0.016350 -3.315 0.001035 ** ## rho_unem -0.010869 0.002935 -3.704 0.000255 *** ## rho_pop 1.660955 0.089211 18.618 &lt; 2e-16 *** ## factor(rho_month)1.17850742580563 -0.001784 0.005161 -0.346 0.729847 ## factor(rho_month)1.35701485161126 0.185167 0.005400 34.292 &lt; 2e-16 *** ## factor(rho_month)1.53552227741689 0.058928 0.006011 9.803 &lt; 2e-16 *** ## factor(rho_month)1.71402970322252 0.111988 0.005249 21.336 &lt; 2e-16 *** ## factor(rho_month)1.89253712902815 0.071313 0.004724 15.096 &lt; 2e-16 *** ## factor(rho_month)2.07104455483379 0.096509 0.005062 19.064 &lt; 2e-16 *** ## factor(rho_month)2.24955198063942 0.072352 0.005462 13.246 &lt; 2e-16 *** ## factor(rho_month)2.42805940644505 -0.014690 0.005493 -2.675 0.007911 ** ## factor(rho_month)2.60656683225068 0.098120 0.005325 18.426 &lt; 2e-16 *** ## factor(rho_month)2.78507425805631 -0.005641 0.005067 -1.113 0.266520 ## factor(rho_month)2.96358168386194 0.061544 0.005106 12.053 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.01596 on 288 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.937, Adjusted R-squared: 0.934 ## F-statistic: 306.1 on 14 and 288 DF, p-value: &lt; 2.2e-16 10.8 Estimate a dynamic model of miles driven using control variables from above. Discuss key differences. ## Distributed Lag Model ## Creating lagged miles variable dta$lagmile &lt;- dplyr::lag(dta$logMilesNA, ordered_by = dta$year) reg.2.dyn &lt;- lm(logMilesNA ~ lagmile + logGasReal + unem + logPop + factor(month) , data=dta) summary(reg.2.dyn) ## ## Call: ## lm(formula = logMilesNA ~ lagmile + logGasReal + unem + logPop + ## factor(month), data = dta) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.070624 -0.010142 0.000926 0.010416 0.063846 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.9098039 0.3676106 -5.195 3.89e-07 *** ## lagmile 0.8064020 0.0327669 24.610 &lt; 2e-16 *** ## logGasReal -0.0152727 0.0052462 -2.911 0.00388 ** ## unem -0.0025328 0.0007938 -3.191 0.00158 ** ## logPop 0.3376747 0.0570442 5.920 9.20e-09 *** ## factor(month)2 0.0050743 0.0049789 1.019 0.30899 ## factor(month)3 0.1910344 0.0058404 32.709 &lt; 2e-16 *** ## factor(month)4 0.0696939 0.0046687 14.928 &lt; 2e-16 *** ## factor(month)5 0.1185358 0.0046908 25.270 &lt; 2e-16 *** ## factor(month)6 0.0765985 0.0053129 14.417 &lt; 2e-16 *** ## factor(month)7 0.1062099 0.0052163 20.361 &lt; 2e-16 *** ## factor(month)8 0.0840785 0.0056219 14.956 &lt; 2e-16 *** ## factor(month)9 -0.0032845 0.0055601 -0.591 0.55517 ## factor(month)10 0.1083157 0.0045332 23.894 &lt; 2e-16 *** ## factor(month)11 0.0035112 0.0047687 0.736 0.46216 ## factor(month)12 0.0696620 0.0045537 15.298 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.0159 on 287 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.9856, Adjusted R-squared: 0.9848 ## F-statistic: 1305 on 15 and 287 DF, p-value: &lt; 2.2e-16 ## Saving the Residuals residual_errors &lt;- resid(reg.2.dyn) ## Regressing Residuals lag_err &lt;- c(NA, residual_errors[1:(length(residual_errors)-1)]) ## Residuals from Dynamic Model plot(lag_err, residual_errors, pch=20, xlab = &quot;Lagged Residuals&quot;, ylab = &quot;Residuals&quot;, bty=&quot;n&quot;, col=&quot;dark blue&quot;, las=1) 10.9 Run an Augmented Dickey-Fuller Test on logMilesNA. ## Change in miles delta_miles &lt;- dta$logMilesNA - dta$lagmile ## Lagged Change in Miles lag_delta_miles &lt;- c(NA, delta_miles[1:(length(dta$lagmile)-1)]) ## Augmented Dickey-Fuller Test aug_dickey_fuller &lt;- lm(delta_miles ~ dta$lagmile + dta$year + lag_delta_miles) summary(aug_dickey_fuller) ## ## Call: ## lm(formula = delta_miles ~ dta$lagmile + dta$year + lag_delta_miles) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.14060 -0.05492 0.01217 0.03746 0.13302 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -4.2949735 1.1607722 -3.700 0.000257 *** ## dta$lagmile -0.2800211 0.0454740 -6.158 2.38e-09 *** ## dta$year 0.0038698 0.0007822 4.947 1.26e-06 *** ## lag_delta_miles -0.1450016 0.0569038 -2.548 0.011330 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.05804 on 298 degrees of freedom ## (2 observations deleted due to missingness) ## Multiple R-squared: 0.1833, Adjusted R-squared: 0.1751 ## F-statistic: 22.3 on 3 and 298 DF, p-value: 4.705e-13 10.10 OPTIONAL QUESTION: Conduct the same analysis from a, b, d, and g using light truck sales as the dependent variable. (Light truck sales use more gas than cars, so the question is whether gas prices affect the kind of car people buy which will affect gas consumption for the life of the car.) ## The Model reg.lt &lt;- lm(lttrucksales~logGasReal + unem + logPop + factor(month) , data=dta) summary(reg.lt) ## ## Call: ## lm(formula = lttrucksales ~ logGasReal + unem + logPop + factor(month), ## data = dta) ## ## Residuals: ## Min 1Q Median 3Q Max ## -225.99 -44.34 1.04 37.41 360.24 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -21060.028 1197.027 -17.594 &lt; 2e-16 *** ## logGasReal -137.896 24.179 -5.703 2.91e-08 *** ## unem -58.137 3.027 -19.208 &lt; 2e-16 *** ## logPop 1744.238 94.889 18.382 &lt; 2e-16 *** ## factor(month)2 59.998 21.335 2.812 0.00526 ** ## factor(month)3 164.627 21.405 7.691 2.31e-13 *** ## factor(month)4 87.059 21.669 4.018 7.50e-05 *** ## factor(month)5 148.803 21.758 6.839 4.75e-11 *** ## factor(month)6 160.837 21.595 7.448 1.10e-12 *** ## factor(month)7 142.036 21.536 6.595 2.02e-10 *** ## factor(month)8 137.300 21.605 6.355 8.09e-10 *** ## factor(month)9 56.147 21.522 2.609 0.00956 ** ## factor(month)10 53.670 21.480 2.499 0.01302 * ## factor(month)11 19.991 21.357 0.936 0.35004 ## factor(month)12 108.294 21.297 5.085 6.62e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 75.41 on 289 degrees of freedom ## Multiple R-squared: 0.7722, Adjusted R-squared: 0.7612 ## F-statistic: 69.98 on 14 and 289 DF, p-value: &lt; 2.2e-16 ## Saving the Residuals dta$residual_errors &lt;- resid(reg.lt) ## Regressing Residuals lag_err &lt;- c(NA, dta$residual_errors[1:(length(dta$residual_errors)-1)]) plot(lag_err, dta$residual_errors, pch=20, xlab = &quot;Lagged Residuals&quot;, ylab = &quot;Residuals&quot;, bty=&quot;n&quot;, col=&quot;dark blue&quot;, las=1) abline(lm(dta$residual_errors ~ lag_err), lwd=2) ## Plotting Over Time plot(dta$residual_errors, pch=20, xlab = &quot;Time&quot;, las=1, ylab = &quot;Residuals&quot;, bty=&quot;n&quot;, col=&quot;dark blue&quot;) # Creating season category and plotting residual error dta %&gt;% ggplot() + geom_line(aes(x=as.Date(date), y=residual_errors, col=season), size=.8) + scale_color_manual(values=c(&quot;orange&quot;, &quot;green&quot;, &quot;red&quot;, &quot;darkblue&quot;))+ labs(x=&quot;Date&quot;, y=&quot;Residual Errors&quot;) + theme_classic() + theme(panel.grid.major.y=element_line(color=&quot;darkgrey&quot;, linetype=&quot;dotted&quot;), panel.grid.minor.y=element_line(color=&quot;darkgrey&quot;, linetype=&quot;dotted&quot;)) %&gt;% labs(col=&quot;Season&quot;) # Creating season category and plotting residual error dta %&gt;% ggplot() + geom_point(aes(x=as.Date(date), y=residual_errors, col=season)) + scale_color_manual(values=c(&quot;orange&quot;, &quot;green&quot;, &quot;red&quot;, &quot;darkblue&quot;))+ labs(x=&quot;Date&quot;, y=&quot;Residual Errors&quot;) + theme_classic() + theme(panel.grid.major.y=element_line(color=&quot;darkgrey&quot;, linetype=&quot;dotted&quot;), panel.grid.minor.y=element_line(color=&quot;darkgrey&quot;, linetype=&quot;dotted&quot;)) %&gt;% labs(col=&quot;Season&quot;) ## Cochrane-Orcutt Model reg.orc2 &lt;- cochrane.orcutt(reg.lt) summary(reg.orc2) ## Call: ## lm(formula = lttrucksales ~ logGasReal + unem + logPop + factor(month), ## data = dta) ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -21271.4536 2398.4045 -8.869 &lt; 2.2e-16 *** ## logGasReal -148.4413 44.3120 -3.350 0.0009162 *** ## unem -52.5199 6.2014 -8.469 1.286e-15 *** ## logPop 1758.1619 190.3266 9.238 &lt; 2.2e-16 *** ## factor(month)2 60.0153 12.8791 4.660 4.842e-06 *** ## factor(month)3 165.6953 16.7822 9.873 &lt; 2.2e-16 *** ## factor(month)4 91.0501 19.7436 4.612 6.016e-06 *** ## factor(month)5 152.7067 21.0327 7.260 3.611e-12 *** ## factor(month)6 162.2202 20.8922 7.765 1.442e-13 *** ## factor(month)7 143.1172 20.8265 6.872 3.920e-11 *** ## factor(month)8 139.6807 20.9328 6.673 1.285e-10 *** ## factor(month)9 60.0386 20.7456 2.894 0.0040938 ** ## factor(month)10 57.8695 19.6575 2.944 0.0035051 ** ## factor(month)11 23.3824 17.4432 1.340 0.1811433 ## factor(month)12 111.1238 13.9218 7.982 3.451e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 57.3085 on 298 degrees of freedom ## Multiple R-squared: 0.6307 , Adjusted R-squared: 0.6258 ## F-statistic: 35.1 on 4 and 298 DF, p-value: &lt; 4.603e-54 ## ## Durbin-Watson statistic ## (original): 0.72932 , p-value: 1.646e-28 ## (transformed): 2.33482 , p-value: 9.971e-01 "],["optional-lab-i-spatial-data-analysis.html", "11 4/3 | Optional Lab I: Spatial Data Analysis 11.1 Start by registering for a census API key here 11.2 Let’s create a map of the distribution of income across DC at the census tract level. We will use the 2015-2019 ACS. Edit the following code to bring in the data with get_acs and create the map 11.3 Do the same for two different states and cities. 11.4 Now visualize income across the entire county at the state level. Include shift_geo = TRUE as an argument 11.5 Do the same at the county level 11.6 Now use get_idb() to visualize the fertility rate around the world", " 11 4/3 | Optional Lab I: Spatial Data Analysis ## Packages library(tidyverse) library(sf) library(tidycensus) library(idbr) 11.1 Start by registering for a census API key here ## Loading Census API Key census_api_key(&quot; &quot;) 11.2 Let’s create a map of the distribution of income across DC at the census tract level. We will use the 2015-2019 ACS. Edit the following code to bring in the data with get_acs and create the map dc_income &lt;- get_acs( geography = &quot;tract&quot;, variables = &quot;B19013_001E&quot;, state = 11, geometry = TRUE, output = &quot;wide&quot;, year = 2019, progress = FALSE) dc_income %&gt;% ggplot() + geom_sf(aes(fill = B19013_001E)) + theme_void() + labs(title = &quot;DC Median Income Distribution&quot;, fill=&quot;Income $&quot;) 11.3 Do the same for two different states and cities. la_income &lt;- get_acs( geography = &quot;tract&quot;, variables = &quot;B19013_001E&quot;, state = 6, county = 37, geometry = TRUE, output = &quot;wide&quot;, year = 2019, progress = FALSE) la_income %&gt;% ggplot() + geom_sf(aes(fill = B19013_001E)) + theme_void() + labs(title = &quot;LA Median Income Distribution&quot;, fill=&quot;Income $&quot;) balt_income &lt;- get_acs( geography = &quot;tract&quot;, variables = &quot;B19013_001E&quot;, state = 24, county = 510, geometry = TRUE, output = &quot;wide&quot;, year = 2019, progress = FALSE) balt_income %&gt;% ggplot() + geom_sf(aes(fill = B19013_001E)) + theme_void() + labs(title = &quot;Baltimore Median Income Distribution&quot;, fill=&quot;Income $&quot;) 11.4 Now visualize income across the entire county at the state level. Include shift_geo = TRUE as an argument us_income &lt;- get_acs( geography = &quot;state&quot;, variables = &quot;B19013_001&quot;, geometry = TRUE, output = &quot;wide&quot;, year = 2019, shift_geo = TRUE, progress = FALSE) us_income %&gt;% ggplot() + geom_sf(aes(fill = B19013_001E)) + theme_void() + labs(title = &quot;US Median Income Distribution&quot;, fill=&quot;Income $&quot;) 11.5 Do the same at the county level us_income_county &lt;- get_acs( geography = &quot;county&quot;, variables = &quot;B19013_001&quot;, geometry = TRUE, output = &quot;wide&quot;, year = 2019, shift_geo = TRUE, progress = FALSE) us_income_county %&gt;% ggplot() + geom_sf(aes(fill = B19013_001E)) + theme_void() + labs(title = &quot;US Median Income Distribution&quot;, fill=&quot;Income $&quot;) 11.6 Now use get_idb() to visualize the fertility rate around the world f_rate &lt;- get_idb( country = &quot;all&quot;, year = 2021, variables = &quot;tfr&quot;, geometry = TRUE, ) f_rate %&gt;% ggplot(aes(fill = tfr)) + geom_sf() + theme_void() + labs(fill = &quot;Fertility Rate&quot;) "]]
